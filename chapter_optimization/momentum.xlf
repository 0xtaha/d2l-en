<?xml version='1.0' encoding='UTF-8'?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file original="chapter_optimization/momentum.md" source-language="zh-CN" target-language="en-US" datatype="markdown">
    <header>
      <skl>
        <external-file href="chapter_optimization/momentum.skl.md"/>
      </skl>
    </header>
    <body>
      <trans-unit id="1">
        <source xml:lang="zh-CN">动量法</source>
        <target xml:lang="en-US">Momentum</target>
      </trans-unit>
      <trans-unit id="2">
        <source xml:lang="zh-CN">在<bpt id="l2">[</bpt>“梯度下降和随机梯度下降”<ept id="l2">]</ept><bpt id="l3">(</bpt>./gd-sgd.md<ept id="l3">)</ept>一节中我们提到，目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫做最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，自变量的迭代方向仅仅取决于自变量当前位置可能会带来一些问题。</source>
        <target xml:lang="en-US">In the <bpt id="l2">[</bpt>"Gradient Descent and Stochastic Gradient Descent"<ept id="l2">]</ept><bpt id="l3">(</bpt>./gd-sgd.md<ept id="l3">)</ept> section, we mentioned that the gradient of the objective function's independent variable represents the direction of the objective function's fastest descend at the current position of the independent variable. Therefore, gradient descent is also called steepest descent. In each iteration, the gradient descends according to the current position of the independent variable while updating the latter along the current position of the gradient. However, this can lead to problems if the iterative direction of the independent variable relies exclusively on the current position of the independent variable.</target>
      </trans-unit>
      <trans-unit id="3">
        <source xml:lang="zh-CN">梯度下降的问题</source>
        <target xml:lang="en-US">Problems with Gradient Descent</target>
      </trans-unit>
      <trans-unit id="4">
        <source xml:lang="zh-CN">让我们考虑一个输入和输出分别为二维向量$\boldsymbol{x} = <bpt id="3">[</bpt>x_1, x_2<ept id="3">]</ept>^\top$和标量的目标函数$f(\boldsymbol{x})=0.1x_1^2+2x_2^2$。跟<bpt id="l7">[</bpt>“梯度下降和随机梯度下降”<ept id="l7">]</ept><bpt id="l8">(</bpt>./gd-sgd.md<ept id="l8">)</ept>一节中不同，这里将$x_1^2$系数从$1$减小到了$0.1$。下面实现基于这个目标函数的梯度下降，并演示使用学习率为$0.4$时自变量的迭代轨迹。</source>
        <target xml:lang="en-US">Now, we will consider an objective function $f(\boldsymbol{x})=0.1x_1^2+2x_2^2$, whose input and output are a two-dimensional vector $\boldsymbol{x} = <bpt id="3">[</bpt>x_1, x_2<ept id="3">]</ept> and a scalar, respectively. In contrast to the <bpt id="l7">[</bpt>"Gradient Descent and Stochastic Gradient Descent"<ept id="l7">]</ept><bpt id="l8">(</bpt>./gd-sgd.md<ept id="l8">)</ept> section, here, the coefficient $x_1^2$ is reduced from $1$ to $0.1$. We are going to implement gradient descent based on this objective function, and demonstrate the iterative trajectory of the independent variable using the learning rate $0.4.</target>
      </trans-unit>
      <trans-unit id="5">
        <source xml:lang="zh-CN">可以看到，同一位置上，目标函数在竖直方向（$x_2$轴方向）比在水平方向（$x_1$轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这造成自变量在水平方向上朝最优解移动变慢。</source>
        <target xml:lang="en-US">As we can see, at the same position, the slope of the objective function has a larger absolute value in the vertical direction ($x_2$ axis direction) than in the horizontal direction ($x_1$ axis direction). Therefore, given the learning rate, using gradient descent for interaction will cause the independent variable to move more in the vertical direction than in the horizontal one. So we need a small learning rate to prevent the independent variable from overshooting the optimal solution for the objective function in the vertical direction. However, it will cause the independent variable to move slower toward the optimal solution in the horizontal direction.</target>
      </trans-unit>
      <trans-unit id="6">
        <source xml:lang="zh-CN">下面我们试着将学习率调的稍大一点，此时自变量在竖直方向不断越过最优解并逐渐发散。</source>
        <target xml:lang="en-US">Now, we try to make the learning rate slightly larger, so the independent variable will continuously overshoot the optimal solution in the vertical direction and gradually diverge.</target>
      </trans-unit>
      <trans-unit id="7">
        <source xml:lang="zh-CN">动量法</source>
        <target xml:lang="en-US">The Momentum Method</target>
      </trans-unit>
      <trans-unit id="8">
        <source xml:lang="zh-CN">动量法的提出是为了应对梯度下降的上述问题。由于小批量随机梯度下降比梯度下降更为广义，本章后续讨论将沿用<bpt id="l2">[</bpt>“小批量随机梯度下降”<ept id="l2">]</ept><bpt id="l3">(</bpt>minibatch-sgd.md<ept id="l3">)</ept>一节中时间步$t$的小批量随机梯度$\boldsymbol{g}_t$的定义。设时间步$t$的自变量为$\boldsymbol{x}_t$、学习率为$\eta_t$。
在时间步$0$，动量法创建速度变量$\boldsymbol{v}_0$，并将其元素初始化成0。在时间步$t&gt;0$，动量法对每次迭代的步骤做如下修改：</source>
        <target xml:lang="en-US">The momentum method was proposed to solve the gradient descent problem described above. Since mini-batch stochastic gradient descent is more general than gradient descent, the subsequent discussion in this chapter will continue to use the definition for mini-batch stochastic gradient descent $\boldsymbol{g}_t$ at time step $t$ given in the <bpt id="l2">[</bpt>"Mini-batch Stochastic Gradient Descent"<ept id="l2">]</ept><bpt id="l3">(</bpt>minibatch-sgd.md<ept id="l3">)</ept> section. We set the independent variable at time step $t$ to $\boldsymbol{x}_t$ and the learning rate to $\eta_t$.
At time step $0$, momentum creates the velocity variable $\boldsymbol{v}_0$ and initializes its elements to zero. At time step $t&gt;0$, momentum modifies the steps of each iteration as follows:</target>
      </trans-unit>
      <trans-unit id="9">
        <source xml:lang="zh-CN">$$
\begin{aligned}
\boldsymbol{v}<bpt id="4">_</bpt>t %%%ampersand%%%\leftarrow \gamma \boldsymbol{v}<ept id="4">_</ept>{t-1} + \eta_t \boldsymbol{g}_t, \
\boldsymbol{x}<bpt id="11">_</bpt>t %%%ampersand%%%\leftarrow \boldsymbol{x}<ept id="11">_</ept>{t-1} - \boldsymbol{v}_t,
\end{aligned}
$$</source>
        <target xml:lang="en-US">$$
\begin{aligned}
\boldsymbol{v}<bpt id="4">_</bpt>t %%%ampersand%%%\leftarrow \gamma \boldsymbol{v}<ept id="4">_</ept>{t-1} +  \eta_t \boldsymbol{g}_t, \
\boldsymbol{x}<bpt id="11">_</bpt>t %%%ampersand%%%\leftarrow \boldsymbol{x}<ept id="11">_</ept>{t-1} - \boldsymbol{v}_t,
\end{aligned}
$$</target>
      </trans-unit>
      <trans-unit id="10">
        <source xml:lang="zh-CN">其中，动量超参数$\gamma$满足$0 \leq \gamma %%%less-than%%% 1$。当$\gamma=0$时，动量法等价于小批量随机梯度下降。</source>
        <target xml:lang="en-US">Here, the momentum hyperparameter $\gamma$ satisfies $0 \leq \gamma %%%less-than%%% 1$. When $\gamma=0$, momentum is equivalent to a mini-batch SGD.</target>
      </trans-unit>
      <trans-unit id="11">
        <source xml:lang="zh-CN">在解释动量法的数学原理前，让我们先从实验中观察梯度下降在使用动量法后的迭代轨迹。</source>
        <target xml:lang="en-US">Before explaining the mathematical principles behind the momentum method, we should take a look at the iterative trajectory of the gradient descent after using momentum in the experiment.</target>
      </trans-unit>
      <trans-unit id="12">
        <source xml:lang="zh-CN">可以看到使用较小的学习率$\eta=0.4$和动量超参数$\gamma=0.5$时，动量法在竖直方向上的移动更加平滑，且在水平方向上更快逼近最优解。下面使用较大的学习率$\eta=0.6$，此时自变量也不再发散。</source>
        <target xml:lang="en-US">As we can see, when using a smaller learning rate ($\eta=0.4$) and momentum hyperparameter ($\gamma=0.5$), momentum moves more smoothly in the vertical direction and approaches the optimal solution faster in the horizontal direction. Now, when we use a larger learning rate ($\eta=0.6$), the independent variable will no longer diverge.</target>
      </trans-unit>
      <trans-unit id="13">
        <source xml:lang="zh-CN">指数加权移动平均</source>
        <target xml:lang="en-US">Exponentially Weighted Moving Average (EWMA)</target>
      </trans-unit>
      <trans-unit id="14">
        <source xml:lang="zh-CN">为了从数学上理解动量法，让我们先解释指数加权移动平均（exponentially weighted moving average）。给定超参数$0 \leq \gamma %%%less-than%%% 1$，当前时间步$t$的变量$y_t$是上一时间步$t-1$的变量$y_{t-1}$和当前时间步另一变量$x_t$的线性组合：</source>
        <target xml:lang="en-US">In order to understand the momentum method mathematically, we must first explain the exponentially weighted moving average(EWMA). Given hyperparameter $0 \leq \gamma %%%less-than%%% 1$, the variable $y_t$ of the current time step $t$ is the linear combination of variable $y_{t-1}$ from the previous time step $t-1$ and another variable $x_t$ of the current step.</target>
      </trans-unit>
      <trans-unit id="15">
        <source xml:lang="zh-CN">$$y_t = \gamma y_{t-1} + (1-\gamma) x_t.$$</source>
        <target xml:lang="en-US">$$y_t = \gamma y_{t-1} + (1-\gamma) x_t.$$</target>
      </trans-unit>
      <trans-unit id="16">
        <source xml:lang="zh-CN">我们可以对$y_t$展开：</source>
        <target xml:lang="en-US">We can expand $y_t$:</target>
      </trans-unit>
      <trans-unit id="17">
        <source xml:lang="zh-CN">$$
\begin{aligned}
y_t  %%%ampersand%%%= (1-\gamma) x_t + \gamma y_{t-1}\
         %%%ampersand%%%= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + \gamma^2y_{t-2}\
         %%%ampersand%%%= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + (1-\gamma) \cdot \gamma^2x_{t-2} + \gamma^3y_{t-3}\
         %%%ampersand%%%\ldots
\end{aligned}
$$</source>
        <target xml:lang="en-US">$$
\begin{aligned}
y_t  %%%ampersand%%%= (1-\gamma) x_t + \gamma y_{t-1}\
         %%%ampersand%%%= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + \gamma^2y_{t-2}\
         %%%ampersand%%%= (1-\gamma)x_t + (1-\gamma) \cdot \gamma x_{t-1} + (1-\gamma) \cdot \gamma^2x_{t-2} + \gamma^3y_{t-3}\
         %%%ampersand%%%\ldots
\end{aligned}
$$</target>
      </trans-unit>
      <trans-unit id="18">
        <source xml:lang="zh-CN">令$n = 1/(1-\gamma)$，那么 $\left(1-1/n\right)^n = \gamma^{1/(1-\gamma)}$。由于</source>
        <target xml:lang="en-US">Let $n = 1/(1-\gamma)$, so $\left(1-1/n\right)^n = \gamma^{1/(1-\gamma)}$. Because</target>
      </trans-unit>
      <trans-unit id="19">
        <source xml:lang="zh-CN">$$ \lim_{n \rightarrow \infty}  \left(1-\frac{1}{n}\right)^n = \exp(-1) \approx 0.3679,$$</source>
        <target xml:lang="en-US">$$ \lim_{n \rightarrow \infty}  \left(1-\frac{1}{n}\right)^n = \exp(-1) \approx 0.3679,$$</target>
      </trans-unit>
      <trans-unit id="20">
        <source xml:lang="zh-CN">所以当$\gamma \rightarrow 1$时，$\gamma^{1/(1-\gamma)}=\exp(-1)$。例如$0.95^{20} \approx \exp(-1)$。如果把$\exp(-1)$当做一个比较小的数，我们可以在近似中忽略所有含$\gamma^{1/(1-\gamma)}$和比$\gamma^{1/(1-\gamma)}$更高阶的系数的项。例如，当$\gamma=0.95$时，</source>
        <target xml:lang="en-US">when $\gamma \rightarrow 1$, $\gamma^{1/(1-\gamma)}=\exp(-1)$. For example, $0.95^{20} \approx \exp(-1)$. If we treat $\exp(-1)$ as a relatively small number, we can ignore all the terms that have $\gamma^{1/(1-\gamma)}$ or coefficients of higher order than $\gamma^{1/(1-\gamma)}$ in them. For example, when $\gamma=0.95$,</target>
      </trans-unit>
      <trans-unit id="21">
        <source xml:lang="zh-CN">$$y_t \approx 0.05 \sum_{i=0}^{19} 0.95^i x_{t-i}.$$</source>
        <target xml:lang="en-US">$$y_t \approx 0.05 \sum_{i=0}^{19} 0.95^i x_{t-i}.$$</target>
      </trans-unit>
      <trans-unit id="22">
        <source xml:lang="zh-CN">因此，在实际中，我们常常将$y_t$看作是对最近$1/(1-\gamma)$个时间步的$x_t$值的加权平均。例如，当$\gamma = 0.95$时，$y_t$可以被看作是对最近20个时间步的$x_t$值的加权平均；当$\gamma = 0.9$时，$y_t$可以看作是对最近10个时间步的$x_t$值的加权平均。而且，离当前时间步$t$越近的$x_t$值获得的权重越大（越接近1）。</source>
        <target xml:lang="en-US">Therefore, in practice, we often treat $y_t$ as the weighted average of the $x_t$ values from the last $1/(1-\gamma)$ time steps. For example, when $\gamma = 0.95$, $y_t$ can be treated as the weighted average of the $x_t$ values from the last 20 time steps; when $\gamma = 0.9$, $y_t$ can be treated as the weighted average of the $x_t$ values from the last 10 time steps. Additionally, the closer the $x_t$ value is to the current time step $t$, the greater the value's weight (closer to 1).</target>
      </trans-unit>
      <trans-unit id="23">
        <source xml:lang="zh-CN">由指数加权移动平均理解动量法</source>
        <target xml:lang="en-US">Understanding the Momentum Method through EWMA</target>
      </trans-unit>
      <trans-unit id="24">
        <source xml:lang="zh-CN">现在，我们对动量法的速度变量做变形：</source>
        <target xml:lang="en-US">Now, we are going to deform the velocity variable of momentum:</target>
      </trans-unit>
      <trans-unit id="25">
        <source xml:lang="zh-CN">$$\boldsymbol{v}<bpt id="3">_</bpt>t \leftarrow \gamma \boldsymbol{v}<ept id="3">_</ept>{t-1} + (1 - \gamma) \left(\frac{\eta_t}{1 - \gamma} \boldsymbol{g}_t\right).</source>
        <target xml:lang="en-US">$$\boldsymbol{v}<bpt id="3">_</bpt>t \leftarrow \gamma \boldsymbol{v}<ept id="3">_</ept>{t-1} + (1 - \gamma) \left(\frac{\eta_t}{1 - \gamma} \boldsymbol{g}_t\right).</target>
      </trans-unit>
      <trans-unit id="26">
        <source xml:lang="zh-CN">$$</source>
        <target xml:lang="en-US">$$</target>
      </trans-unit>
      <trans-unit id="27">
        <source xml:lang="zh-CN">由指数加权移动平均的形式可得，速度变量$\boldsymbol{v}<bpt id="3">_</bpt>t$实际上对序列$\{\eta<ept id="3">_</ept>{t-i}\boldsymbol{g}_{t-i} /(1-\gamma):i=0,\ldots,1/(1-\gamma)-1}$做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，动量法在每个时间步的自变量更新量近似于将前者对应的最近$1/(1-\gamma)$个时间步的更新量做了指数加权移动平均后再除以$1-\gamma$。所以动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。在本节之前示例的优化问题中，所有梯度在水平方向上为正（向右）、而在竖直方向上时正（向上）时负（向下）。这样，我们就可以使用较大的学习率，从而使自变量向最优解更快移动。</source>
        <target xml:lang="en-US">Based on the EWMA format, we know that the velocity variable $\boldsymbol{v<bpt id="3">_</bpt>t$ has performed an EWMA on the sequence $\{\eta<ept id="3">_</ept>{t-i}\boldsymbol{g}_{t-i} /(1-\gamma):i=0,\ldots,1/(1-\gamma)-1}$. In other words, compared with the mini-batch SGD, the updated momentum value for the independent variable at each time step approximates the performance of EWMA on the updated values from the last $1/(1-\gamma)$ time steps and then dividing the result by $1-\gamma$. Therefore, in the momentum method, the magnitude of the independent variable's movement in all directions depends not only on the current gradient, but also on whether past gradients are consistent in all directions. In the optimization problems described previously, all gradients remain positive in the horizontal direction (to the right), but can be either positive (upward) or negative (downward) in the vertical direction. This allows us to use a larger learning rate to more quickly move the independent variable toward the optimal solution.</target>
      </trans-unit>
      <trans-unit id="28">
        <source xml:lang="zh-CN">从零开始实现</source>
        <target xml:lang="en-US">Implementation from Scratch</target>
      </trans-unit>
      <trans-unit id="29">
        <source xml:lang="zh-CN">相对于小批量随机梯度下降，动量法需要对每一个自变量维护一个同它一样形状的速度变量，且超参数里多了动量超参数。实现中，我们将速度变量用更广义的状态变量<bpt id="2">`</bpt>states<ept id="2">`</ept>表示。</source>
        <target xml:lang="en-US">Compared with mini-batch SGD, the momentum method needs to maintain a velocity variable of the same shape for each independent variable and a momentum hyperparameter is added to the hyperparameter category. In the implementation, we use the state variable <bpt id="2">`</bpt>states<ept id="2">`</ept> to represent the velocity variable in a more general sense.</target>
      </trans-unit>
      <trans-unit id="30">
        <source xml:lang="zh-CN">我们先将动量超参数<bpt id="2">`</bpt>momentum<ept id="2">`</ept>设0.5，这时可以看成是特殊的小批量随机梯度下降：其小批量随机梯度为最近2个时间步的2倍小批量梯度的加权平均。</source>
        <target xml:lang="en-US">When we set the momentum hyperparameter <bpt id="2">`</bpt>momentum<ept id="2">`</ept> to 0.5, it can be treated as a mini-batch SGD: the mini-batch gradient here is the weighted average of twice the mini-batch gradient of the last two time steps.</target>
      </trans-unit>
      <trans-unit id="31">
        <source xml:lang="zh-CN">将动量超参数<bpt id="2">`</bpt>momentum<ept id="2">`</ept>增大到0.9，这时依然可以看成是特殊的小批量随机梯度下降：其小批量随机梯度为最近10个时间步的10倍小批量梯度的加权平均。我们先保持学习率0.02不变。</source>
        <target xml:lang="en-US">When we increase the momentum hyperparameter <bpt id="2">`</bpt>momentum<ept id="2">`</ept> to 0.9, it can still be treated as a mini-batch SGD: the mini-batch gradient here will be the weighted average of ten times the mini-batch gradient of the last 10 time steps. Now we keep the learning rate at 0.02.</target>
      </trans-unit>
      <trans-unit id="32">
        <source xml:lang="zh-CN">可见目标函数值在后期迭代过程中的变化不够平滑。直觉上，10倍小批量梯度比2倍小批量梯度大了5倍，我们可以试着将学习率减小到原来的1/5。此时目标函数值在下降了一段时间后变化更加平滑。</source>
        <target xml:lang="en-US">We can see that the value change of the objective function is not smooth enough at later stages of iteration. Intuitively, ten times the mini-batch gradient is five times larger than two times the mini-batch gradient, so we can try to reduce the learning rate to 1/5 of its original value. Now, the value change of the objective function becomes smoother after its period of decline.</target>
      </trans-unit>
      <trans-unit id="33">
        <source xml:lang="zh-CN">Gluon实现</source>
        <target xml:lang="en-US">Implementation with Gluon</target>
      </trans-unit>
      <trans-unit id="34">
        <source xml:lang="zh-CN">在Gluon中，只需要在<bpt id="2">`</bpt>Trainer<ept id="2">`</ept>实例中通过<bpt id="4">`</bpt>momentum<ept id="4">`</ept>来指定动量超参数即可使用动量法。</source>
        <target xml:lang="en-US">In Gluon, we only need to use <bpt id="4">`</bpt>momentum<ept id="4">`</ept> to define the momentum hyperparameter in the <bpt id="2">`</bpt>Trainer<ept id="2">`</ept> instance to implement momentum.</target>
      </trans-unit>
      <trans-unit id="35">
        <source xml:lang="zh-CN">小结</source>
        <target xml:lang="en-US">Summary</target>
      </trans-unit>
      <trans-unit id="36">
        <source xml:lang="zh-CN">动量法使用了指数加权移动平均的思想。它将过去时间步的梯度做了加权平均，且权重按时间步指数衰减。</source>
        <target xml:lang="en-US">The momentum method uses the EWMA concept. It takes the weighted average of past time steps, with weights that decay exponentially by the time step.</target>
      </trans-unit>
      <trans-unit id="37">
        <source xml:lang="zh-CN">动量法使得相邻时间步的自变量更新在方向上更加一致。</source>
        <target xml:lang="en-US">Momentum makes independent variable updates for adjacent time steps more consistent in direction.</target>
      </trans-unit>
      <trans-unit id="38">
        <source xml:lang="zh-CN">练习</source>
        <target xml:lang="en-US">exercise</target>
      </trans-unit>
      <trans-unit id="39">
        <source xml:lang="zh-CN">使用其他动量超参数和学习率的组合，观察并分析实验结果。</source>
        <target xml:lang="en-US">Use other combinations of momentum hyperparameters and learning rates and observe and analyze the different experimental results.</target>
      </trans-unit>
      <trans-unit id="40">
        <source xml:lang="zh-CN">扫码直达<bpt id="l2">[</bpt>讨论区<ept id="l2">]</ept><bpt id="l3">(</bpt>https://discuss.gluon.ai/t/topic/1879<ept id="l3">)</ept></source>
        <target xml:lang="en-US">Scan the QR Code to Access <bpt id="l2">[</bpt>Discussions<ept id="l2">]</ept><bpt id="l3">(</bpt>https://discuss.gluon.ai/t/topic/1879<ept id="l3">)</ept></target>
      </trans-unit>
      <trans-unit id="41">
        <source xml:lang="zh-CN"><bpt id="1">![</bpt>../img/qr_momentum.svg<ept id="1">]</ept></source>
        <target xml:lang="en-US"><bpt id="1">![</bpt>../img/qr_momentum.svg<ept id="1">]</ept></target>
      </trans-unit>
    </body>
 </file>
</xliff>