<?xml version='1.0' encoding='UTF-8'?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file original="chapter_optimization/minibatch-sgd.md" source-language="zh-CN" target-language="en-US" datatype="markdown">
    <header>
      <skl>
        <external-file href="chapter_optimization/minibatch-sgd.skl.md"/>
      </skl>
    </header>
    <body>
      <trans-unit id="1">
        <source xml:lang="zh-CN">小批量随机梯度下降</source>
        <target xml:lang="en-US">Mini-Batch Stochastic Gradient Descent</target>
      </trans-unit>
      <trans-unit id="2">
        <source xml:lang="zh-CN">在每一次迭代中，梯度下降使用整个训练数据集来计算梯度，因此它有时也被称为批量梯度下降（batch gradient descent）。而随机梯度下降在每次迭代中只随机采样一个样本来计算梯度。正如我们在前几章中所看到的，我们还可以在每轮迭代中随机均匀采样多个样本来组成一个小批量，然后使用这个小批量来计算梯度。下面我们将描述小批量随机梯度下降。</source>
        <target xml:lang="en-US">In each iteration, the gradient descent uses the entire training data set to compute the gradient, so it is sometimes referred to as batch gradient descent. Stochastic gradient descent (SGD) only randomly select one example in each iteration to compute the gradient. Just like in the previous chapters, we can perform random uniform sampling for each iteration to form a mini-batch and then use this mini-batch to compute the gradient. Now, we are going to discuss mini-batch stochastic gradient descent.</target>
      </trans-unit>
      <trans-unit id="3">
        <source xml:lang="zh-CN">设目标函数$f(\boldsymbol{x}): \mathbb{R}^d \rightarrow \mathbb{R}$。在迭代开始前的时间步设为0。该时间步的自变量记为$\boldsymbol{x}_0\in \mathbb{R}^d$，通常由随机初始化得到。在接下来的每一个时间步$t&gt;0$中，小批量随机梯度下降随机均匀采样一个由训练数据样本索引所组成的小批量$\mathcal{B}_t$。我们可以通过重复采样（sampling with replacement）或者不重复采样（sampling without replacement）得到一个小批量中的各个样本。前者允许同一个小批量中出现重复的样本，后者则不允许如此，且更常见。对于这两者间的任一种方式，我们都可以使用</source>
        <target xml:lang="en-US">Set objective function f(\boldsymbol{x}): \mathbb{R}^d \rightarrow \mathbb{R}$. The time step before the start of iteration is set to 0. The independent variable of this time step is $\boldsymbol{x}_0\in \mathbb{R}^d$ and is usually obtained by random initialization. In each subsequent time step $t&gt;0$, mini-batch SGD uses random uniform sampling to get a mini-batch $\mathcal{B}_t$ made of example indices from the training data set. We can use sampling with replacement or sampling without replacement to get a mini-batch example. The former method allows duplicate examples in the same mini-batch, the latter does not and is more commonly used. We can use either of the two methods</target>
      </trans-unit>
      <trans-unit id="4">
        <source xml:lang="zh-CN">$$\boldsymbol{g}<bpt id="3">_</bpt>t \leftarrow \nabla f<ept id="3">_</ept>{\mathcal{B}<bpt id="6">_</bpt>t}(\boldsymbol{x}<ept id="6">_</ept>{t-1}) = \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t}\nabla f_i(\boldsymbol{x}_{t-1})$$</source>
        <target xml:lang="en-US">$$\boldsymbol{g<bpt id="3">_</bpt>t \leftarrow \nabla f<ept id="3">_</ept>{\mathcal{B}<bpt id="6">_</bpt>t}(\boldsymbol{x}<ept id="6">_</ept>{t-1}) = \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t}\nabla f_i(\boldsymbol{x}_{t-1})$$</target>
      </trans-unit>
      <trans-unit id="5">
        <source xml:lang="zh-CN">来计算时间步$t$的小批量$\mathcal{B}<bpt id="3">_</bpt>t$上目标函数位于$\boldsymbol{x}<ept id="3">_</ept>{t-1}$处的梯度$\boldsymbol{g}_t$。这里$|\mathcal{B}|$代表批量大小，即小批量中样本的个数，是一个超参数。同随机梯度一样，重复采样所得的小批量随机梯度$\boldsymbol{g}<bpt id="9">_</bpt>t$也是对梯度$\nabla f(\boldsymbol{x}<ept id="9">_</ept>{t-1})$的无偏估计。给定学习率$\eta_t$（取正数），小批量随机梯度下降对自变量的迭代如下：</source>
        <target xml:lang="en-US">to compute the gradient $\boldsymbol{g}_t$ of the objective function at $\boldsymbol{x}<ept id="3">_</ept>{t-1}$ with mini-batch $\mathcal{B}<bpt id="3">_</bpt>t$ at time step $t$. Here, $|\mathcal{B}|$ is the size of the batch, which is the number of examples in the mini-batch. This is a hyper-parameter. Just like the stochastic gradient, the mini-batch SGD $\boldsymbol{g}<bpt id="9">_</bpt>t$ obtained by sampling with replacement is also the unbiased estimate of the gradient $\nabla f(\boldsymbol{x}<ept id="9">_</ept>{t-1})$. Given the learning rate $\eta_t$ (positive), the iteration of the mini-batch SGD on the independent variable is as follows:</target>
      </trans-unit>
      <trans-unit id="6">
        <source xml:lang="zh-CN">$$\boldsymbol{x}<bpt id="3">_</bpt>t \leftarrow \boldsymbol{x}<ept id="3">_</ept>{t-1} - \eta_t \boldsymbol{g}_t.$$</source>
        <target xml:lang="en-US">$$\boldsymbol{x}<bpt id="3">_</bpt>t \leftarrow \boldsymbol{x}<ept id="3">_</ept>{t-1} - \eta_t \boldsymbol{g}_t.$$</target>
      </trans-unit>
      <trans-unit id="7">
        <source xml:lang="zh-CN">基于随机采样得到的梯度的方差在迭代过程中无法减小，因此在实际中，（小批量）随机梯度下降的学习率可以在迭代过程中自我衰减，例如$\eta_t=\eta t^\alpha$（通常$\alpha=-1$或者$-0.5$）、$\eta_t = \eta \alpha^t$（例如$\alpha=0.95$）或者每迭代若干次后将学习率衰减一次。如此一来，学习率和（小批量）随机梯度乘积的方差会减小。而梯度下降在迭代过程中一直使用目标函数的真实梯度，无需自我衰减学习率。</source>
        <target xml:lang="en-US">The variance of the gradient based on random sampling cannot be reduced during the iterative process, so in practice, the learning rate of the (mini-batch) SGD can self-decay during the iteration, such as $\eta_t=\eta t^\alpha $ (usually $\alpha=-1$ or $-0.5$), $\eta_t = \eta \alpha^t$ (e.g $\alpha=0.95$), or learning rate decay once per iteration or after several iterations. As a result, the variance of the learning rate and the (mini-batch) SGD will decrease. Gradient descent always uses the true gradient of the objective function during the iteration, without the need to self-decay the learning rate.</target>
      </trans-unit>
      <trans-unit id="8">
        <source xml:lang="zh-CN">小批量随机梯度下降中每次迭代的计算开销为$\mathcal{O}(|\mathcal{B}|)$。当批量大小为1时，该算法即为随机梯度下降；当批量大小等于训练数据样本数时，该算法即为梯度下降。当批量较小时，每次迭代中使用的样本少，这会导致并行处理和内存使用效率变低。这使得在计算同样数目样本的情况下比使用更大批量时所花时间更多。当批量较大时，每个小批量梯度里可能含有更多的冗余信息。为了得到较好的解，批量较大时比批量较小时可能需要计算更多数目的样本，例如增大迭代周期数。</source>
        <target xml:lang="en-US">The cost for computing each iteration is $\mathcal{O}(|\mathcal{B}|)$. When the batch size is 1, the algorithm is an SGD; when the batch size equals the example size of the training data, the algorithm is a gradient descent. When the batch size is small, fewer examples are used in each iteration, which will result in parallel processing and reduce the RAM usage efficiency. This makes it more time consuming to compute examples of the same size than using larger batches. When the batch size increases, each mini-batch gradient may contain more redundant information. To get a better solution, we need to compute more examples for a larger batch size, such as increasing the number of epochs.</target>
      </trans-unit>
      <trans-unit id="9">
        <source xml:lang="zh-CN">读取数据</source>
        <target xml:lang="en-US">Reading Data</target>
      </trans-unit>
      <trans-unit id="10">
        <source xml:lang="zh-CN">这一章里我们将使用一个来自NASA的测试不同飞机机翼噪音的数据集来比较各个优化算法 <bpt id="2">[</bpt>1<ept id="2">]</ept>。我们使用该数据集的前1500个样本和5个特征，并使用标准化对数据进行预处理。</source>
        <target xml:lang="en-US">In this chapter, we will use a data set developed by NASA to test the wing noise from different aircraft to compare these optimization algorithms<bpt id="2">[</bpt>1<ept id="2">]</ept>. We will use the first 1500 examples of the data set, 5 features, and a normalization method to preprocess the data.</target>
      </trans-unit>
      <trans-unit id="11">
        <source xml:lang="zh-CN">本函数已保存在 gluonbook 包中方便以后使用。</source>
        <target xml:lang="en-US">This function is saved in the gluonbook package for future use.</target>
      </trans-unit>
      <trans-unit id="12">
        <source xml:lang="zh-CN">从零开始实现</source>
        <target xml:lang="en-US">Implementation from Scratch</target>
      </trans-unit>
      <trans-unit id="13">
        <source xml:lang="zh-CN"><bpt id="l1">[</bpt>“线性回归的从零开始实现”<ept id="l1">]</ept><bpt id="l2">(</bpt>../chapter_deep-learning-basics/linear-regression-scratch.md<ept id="l2">)</ept>一节中已经实现过小批量随机梯度下降算法。我们在这里将它的输入参数变得更加通用，主要是为了方便本章后面介绍的其他优化算法也可以使用同样的输入。具体来说，我们添加了一个状态输入<bpt id="3">`</bpt>states<ept id="3">`</ept>并将超参数放在字典<bpt id="5">`</bpt>hyperparams<ept id="5">`</ept>里。此外，我们将在训练函数里对各个小批量样本的损失求平均，因此优化算法里的梯度不需要除以批量大小。</source>
        <target xml:lang="en-US">We have already implemented the mini-batch SGD algorithm in the <bpt id="l1">[</bpt>Linear Regression Implemented From Scratch<ept id="l1">]</ept><bpt id="l2">(</bpt>../chapter_deep-learning-basics/linear-regression-scratch.md<ept id="l2">)</ept> section. We have made its input parameters more generic here, so that we can conveniently use the same input for the other optimization algorithms introduced later in this chapter. Specifically, we add the status input <bpt id="3">`</bpt>states<ept id="3">`</ept> and place the hyper-parameter in dictionary <bpt id="5">`</bpt>hyperparams<ept id="5">`</ept>. In addition, we will average the loss of each mini-batch example in the training function, so the gradient in the optimization algorithm does not need to be divided by the batch size.</target>
      </trans-unit>
      <trans-unit id="14">
        <source xml:lang="zh-CN">下面实现一个通用的训练函数，以方便本章后面介绍的其他优化算法使用。它初始化一个线性回归模型，然后可以使用小批量随机梯度下降以及后续小节介绍的其它算法来训练模型。</source>
        <target xml:lang="en-US">Next, we are going to implement a generic training function to facilitate the use of the other optimization algorithms introduced later in this chapter. It initializes a linear regression model and can then be used to train the model with the mini-batch SGD and other algorithms introduced in subsequent sections.</target>
      </trans-unit>
      <trans-unit id="15">
        <source xml:lang="zh-CN">本函数已保存在 gluonbook 包中方便以后使用。</source>
        <target xml:lang="en-US">This function is saved in the gluonbook package for future use.</target>
      </trans-unit>
      <trans-unit id="16">
        <source xml:lang="zh-CN">初始化模型。</source>
        <target xml:lang="en-US">Initialize model parameters.</target>
      </trans-unit>
      <trans-unit id="17">
        <source xml:lang="zh-CN">使用平均损失。</source>
        <target xml:lang="en-US">Average the loss.</target>
      </trans-unit>
      <trans-unit id="18">
        <source xml:lang="zh-CN">迭代模型参数。</source>
        <target xml:lang="en-US">Update model parameter(s).</target>
      </trans-unit>
      <trans-unit id="19">
        <source xml:lang="zh-CN">每 100 个样本记录下当前训练误差。</source>
        <target xml:lang="en-US">Record the current training error for every 100 examples.</target>
      </trans-unit>
      <trans-unit id="20">
        <source xml:lang="zh-CN">打印结果和作图。</source>
        <target xml:lang="en-US">Print and plot the results.</target>
      </trans-unit>
      <trans-unit id="21">
        <source xml:lang="zh-CN">当批量大小为样本总数1500时，优化使用的是梯度下降。梯度下降的1个迭代周期对模型参数只迭代1次。可以看到6次迭代后目标函数值（训练损失）的下降趋向了平稳。</source>
        <target xml:lang="en-US">When the batch size equals 1500 (the total number of examples), we use gradient descent for optimization. The model parameters will be iterated only once for each epoch of the gradient descent. As we can see, the downward trend of the value of the objective function (training loss) flattened out after 6 iterations.</target>
      </trans-unit>
      <trans-unit id="22">
        <source xml:lang="zh-CN">当批量大小为1时，优化使用的是随机梯度下降。为了简化实现，有关（小批量）随机梯度下降的实验中，我们未对学习率进行自我衰减，而是直接采用较小的常数学习率。随机梯度下降中，每处理一个样本会更新一次自变量（模型参数），一个迭代周期里会对自变量进行1500次更新。可以看到，目标函数值的下降在1个迭代周期后变得较为平缓。</source>
        <target xml:lang="en-US">When the batch size equals 1, we use SGD for optimization. In order to simplify the implementation, we did not self-decay the learning rate. Instead, we simply used a small constant for the learning rate in the (mini-batch) SGD experiment. In SGD, the independent variable (model parameter) is updated whenever an example is processed. Thus it is updated 1500 times in one epoch. As we can see, the decline in the value of the objective function slows down after one epoch.</target>
      </trans-unit>
      <trans-unit id="23">
        <source xml:lang="zh-CN">虽然随机梯度下降和梯度下降在一个迭代周期里都处理了1500个样本，但实验中随机梯度下降的一个迭代周期耗时更多。这是因为随机梯度下降在一个迭代周期里做了更多次的自变量迭代，而且单样本的梯度计算难以有效利用并行计算。</source>
        <target xml:lang="en-US">Although both the procedures processed 1500 examples within one epoch, SGD consumes more time than gradient descent in our experiment. This is because SGD performed more iterations on the independent variable within one epoch, and it is harder for single-example gradient computation to use parallel computing effectively.</target>
      </trans-unit>
      <trans-unit id="24">
        <source xml:lang="zh-CN">当批量大小为10时，优化使用的是小批量随机梯度下降。它在每个迭代周期的耗时介于梯度下降和随机梯度下降的耗时之间。</source>
        <target xml:lang="en-US">When the batch size equals 10, we use mini-batch SGD for optimization. The time required for one epoch is between the time needed for gradient descent and SGD to complete the same epoch.</target>
      </trans-unit>
      <trans-unit id="25">
        <source xml:lang="zh-CN">Gluon实现</source>
        <target xml:lang="en-US">Implementation with Gluon</target>
      </trans-unit>
      <trans-unit id="26">
        <source xml:lang="zh-CN">在Gluon里我们可以通过<bpt id="2">`</bpt>Trainer<ept id="2">`</ept>类来调用优化算法。下面实现一个通用的训练函数，它通过优化算法的名字<bpt id="4">`</bpt>trainer_name<ept id="4">`</ept>和超参数<bpt id="6">`</bpt>trainer_hyperparams<ept id="6">`</ept>来创建<bpt id="8">`</bpt>Trainer<ept id="8">`</ept>实例。</source>
        <target xml:lang="en-US">In Gluon, we can use the <bpt id="2">`</bpt>Trainer<ept id="2">`</ept> class to call optimization algorithms. Next, we are going to implement a generic training function that uses the optimization name <bpt id="4">`</bpt>trainer name<ept id="4">`</ept> and hyperparameter <bpt id="6">`</bpt>trainer_hyperparameter<ept id="6">`</ept> to create the instance <bpt id="8">`</bpt>Trainer<ept id="8">`</ept>.</target>
      </trans-unit>
      <trans-unit id="27">
        <source xml:lang="zh-CN">本函数已保存在 gluonbook 包中方便以后使用。</source>
        <target xml:lang="en-US">This function is saved in the gluonbook package for future use.</target>
      </trans-unit>
      <trans-unit id="28">
        <source xml:lang="zh-CN">初始化模型。</source>
        <target xml:lang="en-US">Initialize model parameters.</target>
      </trans-unit>
      <trans-unit id="29">
        <source xml:lang="zh-CN">创建 Trainer 实例来迭代模型参数。</source>
        <target xml:lang="en-US">Create the instance "Trainer" to update model parameter(s).</target>
      </trans-unit>
      <trans-unit id="30">
        <source xml:lang="zh-CN">在 Trainer 实例里做梯度平均。</source>
        <target xml:lang="en-US">Average the gradient in the "Trainer" instance.</target>
      </trans-unit>
      <trans-unit id="31">
        <source xml:lang="zh-CN">打印结果和作图。</source>
        <target xml:lang="en-US">Print and plot the results.</target>
      </trans-unit>
      <trans-unit id="32">
        <source xml:lang="zh-CN">使用Gluon重复上一个实验。</source>
        <target xml:lang="en-US">Use Gluon to repeat the last experiment.</target>
      </trans-unit>
      <trans-unit id="33">
        <source xml:lang="zh-CN">小结</source>
        <target xml:lang="en-US">Summary</target>
      </trans-unit>
      <trans-unit id="34">
        <source xml:lang="zh-CN">小批量随机梯度每次随机均匀采样一个小批量的训练样本来计算梯度。</source>
        <target xml:lang="en-US">Mini-batch stochastic gradient uses random uniform sampling to get a mini-batch training example for gradient computation.</target>
      </trans-unit>
      <trans-unit id="35">
        <source xml:lang="zh-CN">在实际中，（小批量）随机梯度下降的学习率可以在迭代过程中自我衰减。</source>
        <target xml:lang="en-US">In practice, learning rates of the (mini-batch) SGD can self-decay during iteration.</target>
      </trans-unit>
      <trans-unit id="36">
        <source xml:lang="zh-CN">通常，小批量随机梯度的每个迭代周期的耗时介于梯度下降和随机梯度下降的耗时之间。</source>
        <target xml:lang="en-US">In general, the time consumption per epoch for mini-batch stochastic gradient is between what takes for gradient descent and SGD to complete the same epoch.</target>
      </trans-unit>
      <trans-unit id="37">
        <source xml:lang="zh-CN">练习</source>
        <target xml:lang="en-US">exercise</target>
      </trans-unit>
      <trans-unit id="38">
        <source xml:lang="zh-CN">修改批量大小和学习率，观察目标函数值的下降速度和每个迭代周期的耗时。</source>
        <target xml:lang="en-US">Modify the batch size and learning rate and observe the rate of decline for the value of the objective function and the time consumed in each epoch.</target>
      </trans-unit>
      <trans-unit id="39">
        <source xml:lang="zh-CN">查阅MXNet文档，使用<bpt id="2">`</bpt>Trainer<ept id="2">`</ept>类的<bpt id="4">`</bpt>set_learning_rate<ept id="4">`</ept>函数，令小批量随机梯度下降的学习率每过一个迭代周期减小到原值的1/10。</source>
        <target xml:lang="en-US">Read the MXNet documentation and use the <bpt id="2">`</bpt>Trainer<ept id="2">`</ept> class <bpt id="4">`</bpt>set_learning_rate<ept id="4">`</ept> function to reduce the learning rate of the mini-batch SGD to 1/10 of its previous value after each epoch.</target>
      </trans-unit>
      <trans-unit id="40">
        <source xml:lang="zh-CN">扫码直达<bpt id="l2">[</bpt>讨论区<ept id="l2">]</ept><bpt id="l3">(</bpt>https://discuss.gluon.ai/t/topic/1877<ept id="l3">)</ept></source>
        <target xml:lang="en-US">Scan the QR Code to Access <bpt id="l2">[</bpt>Discussions<ept id="l2">]</ept><bpt id="l3">(</bpt>https://discuss.gluon.ai/t/topic/1877<ept id="l3">)</ept></target>
      </trans-unit>
      <trans-unit id="41">
        <source xml:lang="zh-CN"><bpt id="1">![</bpt>../img/qr_minibatch-sgd.svg<ept id="1">]</ept></source>
        <target xml:lang="en-US"><bpt id="1">![</bpt>../img/qr_minibatch-sgd.svg<ept id="1">]</ept></target>
      </trans-unit>
      <trans-unit id="42">
        <source xml:lang="zh-CN">参考文献</source>
        <target xml:lang="en-US">Reference</target>
      </trans-unit>
      <trans-unit id="43">
        <source xml:lang="zh-CN"><bpt id="1">[</bpt>1<ept id="1">]</ept> 飞机机翼噪音数据集。https://</source>
        <target xml:lang="en-US"><bpt id="1">[</bpt>1<ept id="1">]</ept> Aircraft wing noise data set. https://</target>
      </trans-unit>
    </body>
 </file>
</xliff>