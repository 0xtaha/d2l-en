<?xml version='1.0' encoding='UTF-8'?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file original="chapter_optimization/adagrad.md" source-language="zh-CN" target-language="en-US" datatype="markdown">
    <header>
      <skl>
        <external-file href="chapter_optimization/adagrad.skl.md"/>
      </skl>
    </header>
    <body>
      <trans-unit id="1">
        <source xml:lang="zh-CN">Adagrad</source>
        <target xml:lang="en-US">Adagrad</target>
      </trans-unit>
      <trans-unit id="2">
        <source xml:lang="zh-CN">在我们之前介绍过的优化算法中，目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。举个例子，假设目标函数为$f$，自变量为一个二维向量$<bpt id="2">[</bpt>x_1, x_2<ept id="2">]</ept>^\top$，该向量中每一个元素在迭代时都使用相同的学习率。例如在学习率为$\eta$的梯度下降中，元素$x_1$和$x_2$都使用相同的学习率$\eta$来自我迭代：</source>
        <target xml:lang="en-US">In the optimization algorithms we introduced previously, each element of the objective function's independent variables uses the same learning rate at the same time step for self-iteration. For example, if we assume that the objective function is $f$ and the independent variable is a two-dimensional vector <bpt id="2">[</bpt>x_1, x_2<ept id="2">]</ept>^\top$, each element in the vector uses the same learning rate when iterating. For example, in gradient descent with the learning rate $\eta$, element $x_1$ and $x_2$ both use the same learning rate $\eta$ for iteration:</target>
      </trans-unit>
      <trans-unit id="3">
        <source xml:lang="zh-CN">$$
x_1 \leftarrow x_1 - \eta \frac{\partial{f}}{\partial{x_1}}, \quad
x_2 \leftarrow x_2 - \eta \frac{\partial{f}}{\partial{x_2}}.</source>
        <target xml:lang="en-US">$$
x_1 \leftarrow x_1 - \eta \frac{\partial{f}}{\partial{x_1}}, \quad
x_2 \leftarrow x_2 - \eta \frac{\partial{f}}{\partial{x_2}}.</target>
      </trans-unit>
      <trans-unit id="4">
        <source xml:lang="zh-CN">$$</source>
        <target xml:lang="en-US">$$</target>
      </trans-unit>
      <trans-unit id="5">
        <source xml:lang="zh-CN">在<bpt id="l2">[</bpt>“动量法”<ept id="l2">]</ept><bpt id="l3">(</bpt>./momentum.md<ept id="l3">)</ept>一节里我们看到当$x_1$和$x_2$的梯度值有较大差别时，我们需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。这一节我们介绍Adagrad算法，它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。</source>
        <target xml:lang="en-US">In the <bpt id="l2">[</bpt>Momentum<ept id="l2">]</ept><bpt id="l3">(</bpt>./momentum.md<ept id="l3">)</ept> section, we can see that, when there is a big difference between the gradient values $x_1$ and $x_2$, a sufficiently small learning rate needs to be selected so that the independent variable will not diverge in the dimension of larger gradient values. However, this will cause the independent variables to iterate too slowly in the dimension with smaller gradient values. The momentum method relies on the exponentially weighted moving average (EWMA) to make the direction of the independent variable more consistent, thus reducing the possibility of divergence. In this section, we are going to introduce Adagrad, an algorithm that adjusts the learning rate according to the gradient value of the independent variable in each dimension to eliminate problems caused when a unified learning rate has to adapt to all dimensions.</target>
      </trans-unit>
      <trans-unit id="6">
        <source xml:lang="zh-CN">算法</source>
        <target xml:lang="en-US">The Algorithm</target>
      </trans-unit>
      <trans-unit id="7">
        <source xml:lang="zh-CN">Adagrad的算法会使用一个小批量随机梯度$\boldsymbol{g}_t$按元素平方的累加变量$\boldsymbol{s}_t$。在时间步0，adagrad将$\boldsymbol{s}_0$中每个元素初始化为0。在时间步$t$，首先将小批量随机梯度$\boldsymbol{g}_t$按元素平方后累加到变量$\boldsymbol{s}_t$：</source>
        <target xml:lang="en-US">The Adadelta algorithm uses the cumulative variable $\boldsymbol{s}_t$ obtained from a square by element operation on the mini-batch stochastic gradient $\boldsymbol{g}_t$. At time step 0, Adagrad initializes each element in $\boldsymbol{s}_0$ to 0. At time step $t$, we first sum the results of the square by element operation for the mini-batch gradient $\boldsymbol{g}_t$ to get the variable $\boldsymbol{s}_t$:</target>
      </trans-unit>
      <trans-unit id="8">
        <source xml:lang="zh-CN">$$\boldsymbol{s}<bpt id="3">_</bpt>t \leftarrow \boldsymbol{s}<ept id="3">_</ept>{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t,$$</source>
        <target xml:lang="en-US">$$\boldsymbol{s}<bpt id="3">_</bpt>t \leftarrow \boldsymbol{s}<ept id="3">_</ept>{t-1} + \boldsymbol{g}_t \odot \boldsymbol{g}_t,$$</target>
      </trans-unit>
      <trans-unit id="9">
        <source xml:lang="zh-CN">其中$\odot$是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：</source>
        <target xml:lang="en-US">Here, $\odot$ is the symbol for multiplication by element. Next, we re-adjust the learning rate of each element in the independent variable of the objective function using element operations:</target>
      </trans-unit>
      <trans-unit id="10">
        <source xml:lang="zh-CN">$$\boldsymbol{x}<bpt id="3">_</bpt>t \leftarrow \boldsymbol{x}<ept id="3">_</ept>{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t,$$</source>
        <target xml:lang="en-US">$$\boldsymbol{x}<bpt id="3">_</bpt>t \leftarrow \boldsymbol{x}<ept id="3">_</ept>{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t,$$</target>
      </trans-unit>
      <trans-unit id="11">
        <source xml:lang="zh-CN">其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，例如$10^{-6}$。这里开方、除法和乘法的运算都是按元素进行的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。</source>
        <target xml:lang="en-US">Here, eta$ is the learning rate while $\epsilon$ is a constant added to maintain numerical stability, such as $10^{-6}$. Here, the square root, division, and multiplication operations are all element operations. Each element in the independent variable of the objective function will have its own learning rate after the operations by elements.</target>
      </trans-unit>
      <trans-unit id="12">
        <source xml:lang="zh-CN">特点</source>
        <target xml:lang="en-US">Features</target>
      </trans-unit>
      <trans-unit id="13">
        <source xml:lang="zh-CN">需要强调的是，小批量随机梯度按元素平方的累加变量$\boldsymbol{s}_t$出现在学习率的分母项中。因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。然而，由于$\boldsymbol{s}_t$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，Adagrad在迭代后期由于学习率过小，可能较难找到一个有用的解。</source>
        <target xml:lang="en-US">We should emphasize that the cumulative variable $\boldsymbol{s}_t$ produced by a square by element operation on the mini-batch stochastic gradient is part of the learning rate denominator. Therefore, if an element in the independent variable of the objective function has a constant and large partial derivative, the learning rate of this element will drop faster. On the contrary, if the partial derivative of such an element remains small, then its learning rate will decline more slowly. However, since $\boldsymbol{s}_t$ accumulates the square by element gradient, the learning rate of each element in the independent variable declines (or remains unchanged) during iteration. Therefore, when the learning rate declines very fast during early iteration, yet the current solution is still not desirable, Adagrad might have difficulty finding a useful solution because the learning rate will be too small at later stages of iteration.</target>
      </trans-unit>
      <trans-unit id="14">
        <source xml:lang="zh-CN">下面我们仍然以目标函数$f(\boldsymbol{x})=0.1x_1^2+2x_2^2$为例观察Adagrad对自变量的迭代轨迹。我们实现Adagrad并使用和上一节实验中相同的学习率0.4。可以看到，自变量的迭代轨迹较平滑。但由于$\boldsymbol{s}_t$的累加效果使得学习率不断衰减，自变量在迭代后期的移动幅度较小。</source>
        <target xml:lang="en-US">Below we will continue to use the objective function $f(\boldsymbol{x})=0.1x_1^2+2x_2^2$ as an example to observe the iterative trajectory of the independent variable in Adagrad. We are going to implement Adagrad using the same learning rate as the experiment in last section, 0.4. As we can see, the iterative trajectory of the independent variable is smoother. However, due to the cumulative effect of $\boldsymbol{s}_t$, the learning rate continuously decays, so the independent variable does not move as much during later stages of iteration.</target>
      </trans-unit>
      <trans-unit id="15">
        <source xml:lang="zh-CN">前两项为自变量梯度。</source>
        <target xml:lang="en-US">The first two terms are the independent variable gradients.</target>
      </trans-unit>
      <trans-unit id="16">
        <source xml:lang="zh-CN">下面增大学习率到$2$。可以看到自变量更为迅速地逼近了最优解。</source>
        <target xml:lang="en-US">Now, we are going to increase the learning rate to $2$. As we can see, the independent variable approaches the optimal solution more quickly.</target>
      </trans-unit>
      <trans-unit id="17">
        <source xml:lang="zh-CN">从零开始实现</source>
        <target xml:lang="en-US">Implementation from Scratch</target>
      </trans-unit>
      <trans-unit id="18">
        <source xml:lang="zh-CN">同动量法一样，Adagrad需要对每个自变量维护同它一样形状的状态变量。我们根据算法中的公式实现Adagrad。</source>
        <target xml:lang="en-US">Like the momentum method, Adagrad needs to maintain a state variable of the same shape for each independent variable. We use the formula from the algorithm to implement Adagrad.</target>
      </trans-unit>
      <trans-unit id="19">
        <source xml:lang="zh-CN">与<bpt id="l2">[</bpt>“小批量随机梯度下降”<ept id="l2">]</ept><bpt id="l3">(</bpt>minibatch-sgd.md<ept id="l3">)</ept>一节中的实验相比，我们在这里使用更大的学习率来训练模型。</source>
        <target xml:lang="en-US">Compared with the experiment in the <bpt id="l2">[</bpt>"Mini-Batch Stochastic Gradient Descent"<ept id="l2">]</ept><bpt id="l3">(</bpt>minibatch-sgd.md<ept id="l3">)</ept> section, here, we use a larger learning rate to train the model.</target>
      </trans-unit>
      <trans-unit id="20">
        <source xml:lang="zh-CN">Gluon实现</source>
        <target xml:lang="en-US">Implementation with Gluon</target>
      </trans-unit>
      <trans-unit id="21">
        <source xml:lang="zh-CN">通过算法名称为“adagrad”的<bpt id="2">`</bpt>Trainer<ept id="2">`</ept>实例，我们便可使用Gluon实现的Adagrad算法来训练模型。</source>
        <target xml:lang="en-US">Using the <bpt id="2">`</bpt>Trainer<ept id="2">`</ept> instance of the algorithm named “adagrad”, we can implement the Adagrad algorithm with Gluon to train models.</target>
      </trans-unit>
      <trans-unit id="22">
        <source xml:lang="zh-CN">小结</source>
        <target xml:lang="en-US">Summary</target>
      </trans-unit>
      <trans-unit id="23">
        <source xml:lang="zh-CN">Adagrad在迭代过程中不断调整学习率，并让目标函数自变量中每个元素都分别拥有自己的学习率。</source>
        <target xml:lang="en-US">Adagrad constantly adjusts the learning rate during iteration to give each element in the independent variable of the objective function its own learning rate.</target>
      </trans-unit>
      <trans-unit id="24">
        <source xml:lang="zh-CN">使用Adagrad时，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。</source>
        <target xml:lang="en-US">When using Adagrad, the learning rate of each element in the independent variable decreases (or remains unchanged) during iteration.</target>
      </trans-unit>
      <trans-unit id="25">
        <source xml:lang="zh-CN">练习</source>
        <target xml:lang="en-US">exercise</target>
      </trans-unit>
      <trans-unit id="26">
        <source xml:lang="zh-CN">在介绍Adagrad的特点时，我们提到了它可能存在的问题。你能想到什么办法来应对这个问题？</source>
        <target xml:lang="en-US">When introducing the features of Adagrad, we mentioned a potential problem. What solutions can you think of to fix this problem?</target>
      </trans-unit>
      <trans-unit id="27">
        <source xml:lang="zh-CN">在实验中尝试使用其他的初始学习率，结果有什么变化？</source>
        <target xml:lang="en-US">Try to use other initial learning rates in the experiment. How does this change the results?</target>
      </trans-unit>
      <trans-unit id="28">
        <source xml:lang="zh-CN">扫码直达<bpt id="l2">[</bpt>讨论区<ept id="l2">]</ept><bpt id="l3">(</bpt>https://discuss.gluon.ai/t/topic/2273<ept id="l3">)</ept></source>
        <target xml:lang="en-US">Scan the QR Code to Access <bpt id="l2">[</bpt>Discussions<ept id="l2">]</ept><bpt id="l3">(</bpt>https://discuss.gluon.ai/t/topic/2273<ept id="l3">)</ept></target>
      </trans-unit>
      <trans-unit id="29">
        <source xml:lang="zh-CN"><bpt id="1">![</bpt>../img/qr_adagrad.svg<ept id="1">]</ept></source>
        <target xml:lang="en-US"><bpt id="1">![</bpt>../img/qr_adagrad.svg<ept id="1">]</ept></target>
      </trans-unit>
      <trans-unit id="30">
        <source xml:lang="zh-CN">参考文献</source>
        <target xml:lang="en-US">Reference</target>
      </trans-unit>
      <trans-unit id="31">
        <source xml:lang="zh-CN"><bpt id="1">[</bpt>1<ept id="1">]</ept> Duchi, J., Hazan, E., %%%ampersand%%% Singer, Y.</source>
        <target xml:lang="en-US"><bpt id="1">[</bpt>1<ept id="1">]</ept> Duchi, J., Hazan, E., %%%ampersand%%% Singer, Y.</target>
      </trans-unit>
      <trans-unit id="32">
        <source xml:lang="zh-CN">(2011).</source>
        <target xml:lang="en-US">(2011).</target>
      </trans-unit>
      <trans-unit id="33">
        <source xml:lang="zh-CN">Adaptive subgradient methods for online learning and stochastic optimization.</source>
        <target xml:lang="en-US">Adaptive subgradient methods for online learning and stochastic optimization.</target>
      </trans-unit>
      <trans-unit id="34">
        <source xml:lang="zh-CN">Journal of Machine Learning Research, 12(Jul), 2121-2159.</source>
        <target xml:lang="en-US">Journal of Machine Learning Research, 12(Jul), 2121-2159.</target>
      </trans-unit>
    </body>
 </file>
</xliff>