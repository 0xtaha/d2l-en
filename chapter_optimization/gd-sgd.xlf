<?xml version='1.0' encoding='UTF-8'?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file original="chapter_optimization/gd-sgd.md" source-language="zh-CN" target-language="en-US" datatype="markdown">
    <header>
      <skl>
        <external-file href="chapter_optimization/gd-sgd.skl.md"/>
      </skl>
    </header>
    <body>
      <trans-unit id="1">
        <source xml:lang="zh-CN">梯度下降和随机梯度下降</source>
        <target xml:lang="en-US">Gradient Descent and Stochastic Gradient Descent</target>
      </trans-unit>
      <trans-unit id="2">
        <source xml:lang="zh-CN">本节中，我们将介绍梯度下降（gradient descent）的工作原理。虽然梯度下降在深度学习中很少被直接使用，但理解梯度的意义以及沿着梯度反方向更新自变量可能降低目标函数值的原因是学习后续优化算法的基础。随后，我们将引出随机梯度下降（stochastic gradient descent）。</source>
        <target xml:lang="en-US">In this section, we are going to introduce the basic principles of gradient descent. Although it is not common for gradient descent to be used directly in deep learning, an understanding of gradients and the reason why the value of an objective function might decline when updating the independent variable along the opposite direction of the gradient is the foundation for future studies on optimization algorithms. Next, we are going to introduce stochastic gradient descent (SGD).</target>
      </trans-unit>
      <trans-unit id="3">
        <source xml:lang="zh-CN">一维梯度下降</source>
        <target xml:lang="en-US">Gradient Descent in One-Dimensional Space</target>
      </trans-unit>
      <trans-unit id="4">
        <source xml:lang="zh-CN">我们先以简单的一维梯度下降为例，解释梯度下降算法可能降低目标函数值的原因。假设连续可导的函数$f: \mathbb{R} \rightarrow \mathbb{R}$的输入和输出都是标量。给定绝对值足够小的数$\epsilon$，根据泰勒展开公式（参见<bpt id="l6">[</bpt>“数学基础”<ept id="l6">]</ept><bpt id="l7">(</bpt>../chapter_appendix/math.md<ept id="l7">)</ept>一节），我们得到以下的近似</source>
        <target xml:lang="en-US">Here, we will use a simple gradient descent in one-dimensional space as an example to explain why the gradient descent algorithm may reduce the value of the objective function. We assume that the input and output of the continuously differentiable function $f: \mathbb{R} \rightarrow \mathbb{R}$ are both scalars. Given $\epsilon$ with a small enough absolute value, according to the Taylor's expansion formula from the <bpt id="l6">[</bpt>"Mathematical basics"<ept id="l6">]</ept><bpt id="l7">(</bpt>../chapter_appendix/math.md<ept id="l7">)</ept> section, we get the following approximation:</target>
      </trans-unit>
      <trans-unit id="5">
        <source xml:lang="zh-CN">$$f(x + \epsilon) \approx f(x) + \epsilon f'(x) .$$</source>
        <target xml:lang="en-US">$$f(x + \epsilon) \approx f(x) + \epsilon f'(x) .$$</target>
      </trans-unit>
      <trans-unit id="6">
        <source xml:lang="zh-CN">这里$f'(x)$是函数$f$在$x$处的梯度。一维函数的梯度是一个标量，也称导数。</source>
        <target xml:lang="en-US">Here, $f'(x)$ is the gradient of function $f$ at $x$. The gradient of a one-dimensional function is a scalar, also known as a derivative.</target>
      </trans-unit>
      <trans-unit id="7">
        <source xml:lang="zh-CN">接下来，找到一个常数$\eta &gt; 0$，使得$\left|\eta f'(x)\right|$足够小，那么可以将$\epsilon$替换为$-\eta f'(x)$并得到</source>
        <target xml:lang="en-US">Next, find a constant $\eta &gt; 0$, to make $\left|\eta f'(x)\right|$ sufficiently small so that we can replace $\epsilon$ with $-\eta f'(x) $ and get</target>
      </trans-unit>
      <trans-unit id="8">
        <source xml:lang="zh-CN">$$f(x - \eta f'(x)) \approx f(x) -  \eta f'(x)^2.$$</source>
        <target xml:lang="en-US">$$f(x - \eta f'(x)) \approx f(x) -  \eta f'(x)^2.$$</target>
      </trans-unit>
      <trans-unit id="9">
        <source xml:lang="zh-CN">如果导数$f'(x) \neq 0$，那么$\eta f'(x)^2&gt;0$，所以</source>
        <target xml:lang="en-US">If the derivative $f'(x) \neq 0$, then $\eta f'(x)^2&gt;0$, so</target>
      </trans-unit>
      <trans-unit id="10">
        <source xml:lang="zh-CN">$$f(x - \eta f'(x)) \lesssim f(x).$$</source>
        <target xml:lang="en-US">$$f(x - \eta f'(x)) \lesssim f(x).$$</target>
      </trans-unit>
      <trans-unit id="11">
        <source xml:lang="zh-CN">这意味着，如果我们通过</source>
        <target xml:lang="en-US">This means that, if we use</target>
      </trans-unit>
      <trans-unit id="12">
        <source xml:lang="zh-CN">$$x \leftarrow x - \eta f'(x)$$</source>
        <target xml:lang="en-US">$$x \leftarrow x - \eta f'(x)$$</target>
      </trans-unit>
      <trans-unit id="13">
        <source xml:lang="zh-CN">来迭代$x$，函数$f(x)$的值可能会降低。因此在梯度下降中，我们先选取一个初始值$x$和常数$\eta &gt; 0$，然后不断通过上式来迭代$x$，直到达到停止条件，例如$f'(x)^2$的值已足够小或迭代次数已达到某个值。</source>
        <target xml:lang="en-US">to iterate $x$, the value of function $f(x)$ might decline. Therefore, in the gradient descent, we first choose an initial value $x$ and a constant $\eta &gt; 0$ and then use them to continuously iterate $x$ until the stop condition is reached, for example, when the value of $f'(x)^2$ is small enough or the number of iterations has reached a certain value.</target>
      </trans-unit>
      <trans-unit id="14">
        <source xml:lang="zh-CN">下面我们以目标函数$f(x)=x^2$为例来看一看梯度下降是如何执行的。虽然我们知道最小化$f(x)$的解为$x=0$，这里我们依然使用这个简单函数来观察$x$是如何被迭代的。首先，导入本节实验所需的包或模块。</source>
        <target xml:lang="en-US">Now we will use the objective function $f(x)=x^2$ as an example to see how gradient descent is implemented. Although we know that $x=0$ is the solution to minimize $f(x)$, here we still use this simple function to observe how $x$ is iterated. First, import the packages or modules required for the experiment in this section.</target>
      </trans-unit>
      <trans-unit id="15">
        <source xml:lang="zh-CN">接下来我们使用$x=10$作为初始值，并设$\eta=0.2$。使用梯度下降对$x$迭代10次，可见最终$x$的值较接近最优解。</source>
        <target xml:lang="en-US">Next, we use $x=10$ as the initial value and assume $\eta=0.2$. Using gradient descent to iterate $x$ 10 times, we can see that, eventually, the value of $x$ approaches the optimal solution.</target>
      </trans-unit>
      <trans-unit id="16">
        <source xml:lang="zh-CN">f(x) = x <bpt id="2">*</bpt> x 的导数为 f'(x) = 2 <ept id="2">*</ept> x。</source>
        <target xml:lang="en-US">f(x) = x<bpt id="2">*</bpt> the derivative of x is f'(x) = 2<ept id="2">*</ept> x.</target>
      </trans-unit>
      <trans-unit id="17">
        <source xml:lang="zh-CN">下面将绘制出自变量$x$的迭代轨迹。</source>
        <target xml:lang="en-US">The iterative trajectory of the independent variable $x$ is plotted as follows.</target>
      </trans-unit>
      <trans-unit id="18">
        <source xml:lang="zh-CN">学习率</source>
        <target xml:lang="en-US">Learning Rate</target>
      </trans-unit>
      <trans-unit id="19">
        <source xml:lang="zh-CN">上述梯度下降算法中的正数$\eta$通常叫做学习率。这是一个超参数，需要人工设定。如果使用过小的学习率，会导致$x$更新缓慢从而需要更多的迭代才能得到较好的解。下面展示了使用学习率$\eta=0.05$时自变量$x$的迭代轨迹。可见，同样迭代10次后，当学习率过小时，最终$x$的值依然与最优解存在较大偏差。</source>
        <target xml:lang="en-US">The positive $\eta$ in the above gradient descent algorithm is usually called the learning rate. This is a hyper-parameter and needs to be set manually. If we use a learning rate that is too small, it will cause $x$ to update at a very slow speed, requiring more iterations to get a better solution. Here, we have the iterative trajectory of the independent variable $x$ with the learning rate $\eta=0.05$. As we can see, after iterating 10 times when the learning rate is too small, there is still a large deviation between the final value of $x$ and the optimal solution.</target>
      </trans-unit>
      <trans-unit id="20">
        <source xml:lang="zh-CN">如果使用过大的学习率，$\left|\eta f'(x)\right|$可能会过大从而使前面提到的一阶泰勒展开公式不再成立：这时我们无法保证迭代$x$会降低$f(x)$的值。举个例子，当我们设学习率$\eta=1.1$时，可以看到$x$不断越过（overshoot）最优解$x=0$并逐渐发散。</source>
        <target xml:lang="en-US">If we use an excessively high learning rate, $\left|\eta f'(x)\right|$ might be too large for the first-order Taylor expansion formula mentioned above to hold. In this case, we cannot guarantee that the iteration of $x$ will be able to lower the value of $f(x)$. For example, when we set the learning rate to $\eta=1.1$, $x$ overshoots the optimal solution $x=0$ and gradually diverges.</target>
      </trans-unit>
      <trans-unit id="21">
        <source xml:lang="zh-CN">多维梯度下降</source>
        <target xml:lang="en-US">Gradient Descent in Multi-Dimensional Space</target>
      </trans-unit>
      <trans-unit id="22">
        <source xml:lang="zh-CN">在了解了一维梯度下降之后，我们再考虑一种更广义的情况：目标函数的输入为向量，输出为标量。假设目标函数$f: \mathbb{R}^d \rightarrow \mathbb{R}$的输入是一个$d$维向量$\boldsymbol{x} = <bpt id="6">[</bpt>x_1, x_2, \ldots, x_d<ept id="6">]</ept>^\top$。目标函数$f(\boldsymbol{x})$有关$\boldsymbol{x}$的梯度是一个由$d$个偏导数组成的向量：</source>
        <target xml:lang="en-US">Now that we understand gradient descent in one-dimensional space, let us consider a more general case: the input of the objective function is a vector and the output is a scalar. We assume that the input of the target function $f: \mathbb{R}^d \rightarrow \mathbb{R}$ is the $d$-dimensional vector $\boldsymbol{x} = <bpt id="6">[</bpt>x_1, x_2, \ldots, x_d<ept id="6">]</ept>^\top$. The gradient of the objective function $f(\boldsymbol{x})$ with respect to $\boldsymbol{x}$ is a vector consisting of $d$ partial derivatives:</target>
      </trans-unit>
      <trans-unit id="23">
        <source xml:lang="zh-CN">$$\nabla_{\boldsymbol{x}} f(\boldsymbol{x}) = \bigg<bpt id="6">[</bpt>\frac{\partial f(\boldsymbol{x})}{\partial x_1}, \frac{\partial f(\boldsymbol{x})}{\partial x_2}, \ldots, \frac{\partial f(\boldsymbol{x})}{\partial x_d}\bigg<ept id="6">]</ept>^\top.$$</source>
        <target xml:lang="en-US">$$\nabla_{\boldsymbol{x}} f(\boldsymbol{x}) = \bigg<bpt id="6">[</bpt>\frac{\partial f(\boldsymbol{x})}{\partial x_1}, \frac{\partial f(\boldsymbol{x})}{\partial x_2}, \ldots, \frac{\partial f(\boldsymbol{x})}{\partial x_d}\bigg<ept id="6">]</ept>^\top.$$</target>
      </trans-unit>
      <trans-unit id="24">
        <source xml:lang="zh-CN">为表示简洁，我们用$\nabla f(\boldsymbol{x})$代替$\nabla_{\boldsymbol{x}} f(\boldsymbol{x})$。梯度中每个偏导数元素$\partial f(\boldsymbol{x})/\partial x_i$代表着$f$在$\boldsymbol{x}$有关输入$x_i$的变化率。为了测量$f$沿着单位向量$\boldsymbol{u}$（即$|\boldsymbol{u}|=1$）方向上的变化率，在多元微积分中，我们定义$f$在$\boldsymbol{x}$上沿着$\boldsymbol{u}$方向的方向导数为</source>
        <target xml:lang="en-US">For simplification, we use $\nabla f(\boldsymbol{x})$ to replace $\nabla_{\boldsymbol{x}} f(\boldsymbol{x})$. Each partial derivative element $\partial f(\boldsymbol{x})/\partial x_i$ in the gradient represents the rate of change for $f$ at $\boldsymbol{x}$ for input $x_i$. In multivariate calculus, to measure the rate of change for $f$ along the unit vector $\boldsymbol{u}$ (i.e. $|\boldsymbol{u}|=1$), we define the directional derivative of $f$ along direction $\boldsymbol{u}$ at $\ Boldsymbol{x}$ to be</target>
      </trans-unit>
      <trans-unit id="25">
        <source xml:lang="zh-CN">$$\text{D}<bpt id="3">_</bpt>{\boldsymbol{u}} f(\boldsymbol{x}) = \lim<ept id="3">_</ept>{h \rightarrow 0}  \frac{f(\boldsymbol{x} + h \boldsymbol{u}) - f(\boldsymbol{x})}{h}.$$</source>
        <target xml:lang="en-US">$$\text{D}<bpt id="3">_</bpt>{\boldsymbol{u}} f(\boldsymbol{x}) = \lim<ept id="3">_</ept>{h \rightarrow 0}  \frac{f(\boldsymbol{x} + h \boldsymbol{u}) - f(\boldsymbol{x})}{h}.$$</target>
      </trans-unit>
      <trans-unit id="26">
        <source xml:lang="zh-CN">依据方向导数性质 <bpt id="2">[</bpt>1，14.6节定理三<ept id="2">]</ept>，以上的方向导数可以改写为</source>
        <target xml:lang="en-US">According to the property of the directional derivative <bpt id="2">[</bpt>1, Section 14.6 Theorem III<ept id="2">]</ept>, the directional derivative above can be rewritten as</target>
      </trans-unit>
      <trans-unit id="27">
        <source xml:lang="zh-CN">$$\text{D}_{\boldsymbol{u}} f(\boldsymbol{x}) = \nabla f(\boldsymbol{x}) \cdot \boldsymbol{u}.$$</source>
        <target xml:lang="en-US">$$\text{D}_{\boldsymbol{u}} f(\boldsymbol{x}) = \nabla f(\boldsymbol{x}) \cdot \boldsymbol{u}.$$</target>
      </trans-unit>
      <trans-unit id="28">
        <source xml:lang="zh-CN">方向导数$\text{D}<bpt id="3">_</bpt>{\boldsymbol{u}} f(\boldsymbol{x})$给出了$f$在$\boldsymbol{x}$上沿着所有可能方向的变化率。为了最小化$f$，我们希望找到$f$能被降低最快的方向。因此，我们可以通过单位向量$\boldsymbol{u}$来最小化方向导数$\text{D}<ept id="3">_</ept>{\boldsymbol{u}} f(\boldsymbol{x})$。</source>
        <target xml:lang="en-US">The directional derivative $$\text{D}<bpt id="3">_</bpt>{\boldsymbol{u}} f(\boldsymbol{x}) gives all the possible rates of change for $f$ along $\boldsymbol{x}$. In order to minimize $f$, we hope to find the direction the will allow us to reduce $f$ in the fastest way. Therefore, we can use the unit vector $\boldsymbol{u}$ to minimize the directional derivative $\text{D}<ept id="3">_</ept>{\boldsymbol{u}} f(\boldsymbol{x})$.</target>
      </trans-unit>
      <trans-unit id="29">
        <source xml:lang="zh-CN">由于$\text{D}<bpt id="3">_</bpt>{\boldsymbol{u}} f(\boldsymbol{x}) = \|\nabla f(\boldsymbol{x})\| \cdot \|\boldsymbol{u}\|  \cdot \text{cos} (\theta) = \|\nabla f(\boldsymbol{x})\|  \cdot \text{cos} (\theta)$，
其中$\theta$为梯度$\nabla f(\boldsymbol{x})$和单位向量$\boldsymbol{u}$之间的夹角，当$\theta = \pi$时，$\text{cos}(\theta)$取得最小值$-1$。因此，当$\boldsymbol{u}$在梯度方向$\nabla f(\boldsymbol{x})$的相反方向时，方向导数$\text{D}<ept id="3">_</ept>{\boldsymbol{u}} f(\boldsymbol{x})$被最小化。所以，我们可能通过梯度下降算法来不断降低目标函数$f$的值：</source>
        <target xml:lang="en-US">For $\text{D}<bpt id="3">_</bpt>{\boldsymbol{u}} f(\boldsymbol{x}) = \|\nabla f(\boldsymbol{x})\| \cdot \|\boldsymbol{u}\|  \cdot \text{cos} (\theta) = \|\nabla f(\boldsymbol{x})\|  \cdot \text{cos} (\theta)$,
Here, $\theta$ is the angle between the gradient $\nabla f(\boldsymbol{x})$ and the unit vector $\boldsymbol{u}$. When $\theta = \pi$, $\text{cos }(\theta)$ gives us the minimum value $-1$. So when $\boldsymbol{u}$ is in a direction that is opposite to the gradient direction $\nabla f(\boldsymbol{x})$, the direction derivative $\text{D}<ept id="3">_</ept>{\boldsymbol{u}} f(\boldsymbol{x})$ is minimized. Therefore, we may continue to reduce the value of objective function $f$ by the gradient descent algorithm:</target>
      </trans-unit>
      <trans-unit id="30">
        <source xml:lang="zh-CN">$$\boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f(\boldsymbol{x}).$$</source>
        <target xml:lang="en-US">$$\boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f(\boldsymbol{x}).$$</target>
      </trans-unit>
      <trans-unit id="31">
        <source xml:lang="zh-CN">相同地，其中$\eta$（取正数）称作学习率。</source>
        <target xml:lang="en-US">Similarly, $\eta$ (positive) is called the learning rate.</target>
      </trans-unit>
      <trans-unit id="32">
        <source xml:lang="zh-CN">下面我们构造一个输入为二维向量$\boldsymbol{x} = <bpt id="3">[</bpt>x_1, x_2<ept id="3">]</ept>^\top$和输出为标量的目标函数$f(\boldsymbol{x})=x_1^2+2x_2^2$。那么，梯度$\nabla f(\boldsymbol{x}) = <bpt id="9">[</bpt>2x_1, 4x_2<ept id="9">]</ept>^\top$。我们将观察梯度下降从初始位置$<bpt id="12">[</bpt>5,2<ept id="12">]</ept>$开始对自变量$\boldsymbol{x}$的迭代轨迹。我们先定义两个辅助函数。第一个函数使用给定的自变量更新函数，从初始位置$<bpt id="15">[</bpt>5,2<ept id="15">]</ept>$开始迭代自变量$\boldsymbol{x}$共20次。第二个函数将可视化自变量$\boldsymbol{x}$的迭代轨迹。</source>
        <target xml:lang="en-US">Now we are going to construct an objective function $f(\boldsymbol{x})=x_1^2+2x_2^2$ with a two-dimensional vector $\boldsymbol{x} = <bpt id="3">[</bpt>x_1, x_2<ept id="3">]</ept>^\top$ as input and a scalar as the output. So we have the gradient $\nabla f(\boldsymbol{x}) = <bpt id="9">[</bpt>2x_1, 4x_2<ept id="9">]</ept>^\top$. We will observe the iterative trajectory of independent variable $\boldsymbol{x}$ by gradient descent from the initial position $<bpt id="12">[</bpt>5,2<ept id="12">]</ept>$. First, we are going to define two helper functions. The first helper uses the given independent variable update function to iterate independent variable $\boldsymbol{x}$ a total of 20 times from the initial position $<bpt id="15">[</bpt>5,2<ept id="15">]</ept>$. The second helper will visualize the iterative trajectory of independent variable $\boldsymbol{x}$.</target>
      </trans-unit>
      <trans-unit id="33">
        <source xml:lang="zh-CN">本函数将保存在 gluonbook 包中方便以后使用。</source>
        <target xml:lang="en-US">This function is saved in the gluonbook package for future use.</target>
      </trans-unit>
      <trans-unit id="34">
        <source xml:lang="zh-CN">s1 和 s2 是自变量状态，之后章节会使用。</source>
        <target xml:lang="en-US">s1 and s2 are states of the independent variable and will be used later in the chapter.</target>
      </trans-unit>
      <trans-unit id="35">
        <source xml:lang="zh-CN">本函数将保存在 gluonbook 包中方便以后使用。</source>
        <target xml:lang="en-US">This function is saved in the gluonbook package for future use.</target>
      </trans-unit>
      <trans-unit id="36">
        <source xml:lang="zh-CN">然后，观察学习率为$0.1$时自变量的迭代轨迹。使用梯度下降对自变量$\boldsymbol{x}$迭代20次后，可见最终$\boldsymbol{x}$的值较接近最优解$<bpt id="4">[</bpt>0,0<ept id="4">]</ept>$。</source>
        <target xml:lang="en-US">Next, we observe the iterative trajectory of the independent variable at learning rate $0.1$. After iterating the independent variable $\boldsymbol{x}$ 20 times using gradient descent, we can see that. eventually, the value of $\boldsymbol{x}$ approaches the optimal solution $<bpt id="4">[</bpt>0, 0<ept id="4">]</ept>$.</target>
      </trans-unit>
      <trans-unit id="37">
        <source xml:lang="zh-CN">目标函数。</source>
        <target xml:lang="en-US">Objective function.</target>
      </trans-unit>
      <trans-unit id="38">
        <source xml:lang="zh-CN">随机梯度下降</source>
        <target xml:lang="en-US">Stochastic Gradient Descent (SGD)</target>
      </trans-unit>
      <trans-unit id="39">
        <source xml:lang="zh-CN">在深度学习里，目标函数通常是训练数据集中有关各个样本的损失函数的平均。设$f_i(\boldsymbol{x})$是有关索引为$i$的训练数据样本的损失函数，$n$是训练数据样本数，$\boldsymbol{x}$是模型的参数向量，那么目标函数定义为</source>
        <target xml:lang="en-US">In deep learning, the objective function is usually the average of the loss functions for each example in the training data set. We assume that $f_i(\boldsymbol{x})$ is the loss function of the training data instance with $n$ examples, an index of $i$, and parameter vector of $\boldsymbol{x}$, then we have the objective function</target>
      </trans-unit>
      <trans-unit id="40">
        <source xml:lang="zh-CN">$$f(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n f_i(\boldsymbol{x}).$$</source>
        <target xml:lang="en-US">$$f(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n f_i(\boldsymbol{x}).$$</target>
      </trans-unit>
      <trans-unit id="41">
        <source xml:lang="zh-CN">目标函数在$\boldsymbol{x}$处的梯度计算为</source>
        <target xml:lang="en-US">The gradient of the objective function at $\boldsymbol{x}$ is computed as</target>
      </trans-unit>
      <trans-unit id="42">
        <source xml:lang="zh-CN">$$\nabla f(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\boldsymbol{x}).$$</source>
        <target xml:lang="en-US">$$\nabla f(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\boldsymbol{x}).$$</target>
      </trans-unit>
      <trans-unit id="43">
        <source xml:lang="zh-CN">如果使用梯度下降，每次自变量迭代的计算开销为$\mathcal{O}(n)$，它随着$n$线性增长。因此，当训练数据样本数很大时，梯度下降每次迭代的计算开销很高。</source>
        <target xml:lang="en-US">If gradient descent is used, the computing cost for each independent variable iteration is $\mathcal{O}(n)$, which grows linearly with $n$. Therefore, when the model training data instance is large, the cost of gradient descent for each iteration will be very high.</target>
      </trans-unit>
      <trans-unit id="44">
        <source xml:lang="zh-CN">随机梯度下降（stochastic gradient descent，简称SGD）减少了每次迭代的计算开销。在随机梯度下降的每次迭代中，我们随机均匀采样的一个样本索引$i\in{1,\ldots,n}$，并计算梯度$\nabla f_i(\boldsymbol{x})$来迭代$\boldsymbol{x}$：</source>
        <target xml:lang="en-US">By using Stochastic gradient descent (SGD), we can reduce these costs. For each iteration in the gradient descent, we use the example index $i\in{1,\ldots,n}$ obtained from random uniform sampling and compute the gradient $\nabla f_i(\boldsymbol{x})$ for the iteration of $\boldsymbol{x}$:</target>
      </trans-unit>
      <trans-unit id="45">
        <source xml:lang="zh-CN">$$\boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f_i(\boldsymbol{x}).$$</source>
        <target xml:lang="en-US">$$\boldsymbol{x} \leftarrow \boldsymbol{x} - \eta \nabla f_i(\boldsymbol{x}).$$</target>
      </trans-unit>
      <trans-unit id="46">
        <source xml:lang="zh-CN">这里$\eta$同样是学习率。可以看到每次迭代的计算开销从梯度下降的$\mathcal{O}(n)$降到了常数$\mathcal{O}(1)$。值得强调的是，随机梯度$\nabla f_i(\boldsymbol{x})$是对梯度$\nabla f(\boldsymbol{x})$的无偏估计：</source>
        <target xml:lang="en-US">Here, $\eta$ is the learning rate. We can see that the computing cost for each iteration drops from $\mathcal{O}(n)$ of the gradient descent to the constant $\mathcal{O}(1)$. We should mention that the stochastic gradient $\nabla f_i(\boldsymbol{x})$ is the unbiased estimate of gradient $\nabla f(\boldsymbol{x})$.</target>
      </trans-unit>
      <trans-unit id="47">
        <source xml:lang="zh-CN">$$\mathbb{E}_i \nabla f_i(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\boldsymbol{x}) = \nabla f(\boldsymbol{x}).$$</source>
        <target xml:lang="en-US">$$\mathbb{E}_i \nabla f_i(\boldsymbol{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\boldsymbol{x}) = \nabla f(\boldsymbol{x}).$$</target>
      </trans-unit>
      <trans-unit id="48">
        <source xml:lang="zh-CN">这意味着，平均来说，随机梯度是对梯度的一个良好的估计。</source>
        <target xml:lang="en-US">This means that, on average, the stochastic gradient is a good estimate of the gradient.</target>
      </trans-unit>
      <trans-unit id="49">
        <source xml:lang="zh-CN">下面我们通过在梯度中添加均值为0的随机噪音来模拟随机梯度下降，以此来比较它与梯度下降的区别。</source>
        <target xml:lang="en-US">Now, we will compare it to gradient descent by adding random noise with a mean of 0 to the gradient to simulate ab SGD.</target>
      </trans-unit>
      <trans-unit id="50">
        <source xml:lang="zh-CN">可以看到，随机梯度下降中自变量的迭代轨迹相对于梯度下降中的来说更为曲折。这是由于实验所添加的噪音使得模拟的随机梯度的准确度下降。在实际中，这些噪音通常来自于训练数据集中的各个样本。</source>
        <target xml:lang="en-US">As we can see, the iterative trajectory of the independent variable in the SGD is more tortuous than in the gradient descent. This is due to the noise added in the experiment, which reduced the accuracy of the simulated stochastic gradient. In practice, such noise usually comes from individual examples in the training data set.</target>
      </trans-unit>
      <trans-unit id="51">
        <source xml:lang="zh-CN">小结</source>
        <target xml:lang="en-US">Summary</target>
      </trans-unit>
      <trans-unit id="52">
        <source xml:lang="zh-CN">使用适当的学习率，沿着梯度反方向更新自变量可能降低目标函数值。梯度下降重复这一更新过程直到得到满足要求的解。</source>
        <target xml:lang="en-US">If we use a more suitable learning rate and update the independent variable in the opposite direction of the gradient, the value of the objective function might be reduced. Gradient descent repeats this update process until a solution that meets the requirements is obtained.</target>
      </trans-unit>
      <trans-unit id="53">
        <source xml:lang="zh-CN">学习率过大过小都有问题。一个合适的学习率通常是需要通过多次实验找到的。</source>
        <target xml:lang="en-US">Problems occur when the learning rate is tool small or too large. A suitable learning rate is usually found only after multiple experiments.</target>
      </trans-unit>
      <trans-unit id="54">
        <source xml:lang="zh-CN">当训练数据集的样本较多时，梯度下降每次迭代计算开销较大，因而随机梯度下降通常更受青睐。</source>
        <target xml:lang="en-US">When there are more examples in the training data set, it costs more to compute each iteration for gradient descent, so SGD is preferred in these cases.</target>
      </trans-unit>
      <trans-unit id="55">
        <source xml:lang="zh-CN">练习</source>
        <target xml:lang="en-US">exercise</target>
      </trans-unit>
      <trans-unit id="56">
        <source xml:lang="zh-CN">使用一个不同的目标函数，观察梯度下降和随机梯度下降中自变量的迭代轨迹。</source>
        <target xml:lang="en-US">Using a different objective function, observe the iterative trajectory of the independent variable in gradient descent and the SGD.</target>
      </trans-unit>
      <trans-unit id="57">
        <source xml:lang="zh-CN">在二维梯度下降的实验中尝试使用不同的学习率，观察并分析实验现象。</source>
        <target xml:lang="en-US">In the experiment for gradient descent in two-dimensional space, try to use different learning rates to observe and analyze the experimental phenomena.</target>
      </trans-unit>
      <trans-unit id="58">
        <source xml:lang="zh-CN">扫码直达<bpt id="l2">[</bpt>讨论区<ept id="l2">]</ept><bpt id="l3">(</bpt>https://discuss.gluon.ai/t/topic/1877<ept id="l3">)</ept></source>
        <target xml:lang="en-US">Scan the QR Code to Access <bpt id="l2">[</bpt>Discussions<ept id="l2">]</ept><bpt id="l3">(</bpt>https://discuss.gluon.ai/t/topic/1877<ept id="l3">)</ept></target>
      </trans-unit>
      <trans-unit id="59">
        <source xml:lang="zh-CN"><bpt id="1">![</bpt>../img/qr_gd-sgd.svg<ept id="1">]</ept></source>
        <target xml:lang="en-US"><bpt id="1">![</bpt>../img/qr_gd-sgd.svg<ept id="1">]</ept></target>
      </trans-unit>
    </body>
 </file>
</xliff>