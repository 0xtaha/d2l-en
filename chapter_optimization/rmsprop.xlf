<?xml version='1.0' encoding='UTF-8'?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file original="chapter_optimization/rmsprop.md" source-language="zh-CN" target-language="en-US" datatype="markdown">
    <header>
      <skl>
        <external-file href="chapter_optimization/rmsprop.skl.md"/>
      </skl>
    </header>
    <body>
      <trans-unit id="1">
        <source xml:lang="zh-CN">RMSProp</source>
        <target xml:lang="en-US">RMSProp</target>
      </trans-unit>
      <trans-unit id="2">
        <source xml:lang="zh-CN">我们在<bpt id="l2">[</bpt>“Adagrad”<ept id="l2">]</ept><bpt id="l3">(</bpt>adagrad.md<ept id="l3">)</ept>一节里提到，由于调整学习率时分母上的变量$\boldsymbol{s}_t$一直在累加按元素平方的小批量随机梯度，目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，Adagrad在迭代后期由于学习率过小，可能较难找到一个有用的解。为了应对这一问题，RMSProp算法对Adagrad做了一点小小的修改 <bpt id="6">[</bpt>1<ept id="6">]</ept>。</source>
        <target xml:lang="en-US">In the experiment in the <bpt id="l2">[</bpt>"Adagrad"<ept id="l2">]</ept><bpt id="l3">(</bpt>adagrad.md<ept id="l3">)</ept> section, the learning rate of each element in the independent variable of the objective function declines (or remains unchanged) during iteration because the variable $\boldsymbol{s}_t$ in the denominator is increased by the square by element operation of the mini-batch stochastic gradient, adjusting the learning rate. Therefore, when the learning rate declines very fast during early iteration, yet the current solution is still not desirable, Adagrad might have difficulty finding a useful solution because the learning rate will be too small at later stages of iteration. To tackle this problem, the RMSProp algorithm made a small modification to Adagrad<bpt id="6">[</bpt>1<ept id="6">]</ept>.</target>
      </trans-unit>
      <trans-unit id="3">
        <source xml:lang="zh-CN">算法</source>
        <target xml:lang="en-US">The Algorithm</target>
      </trans-unit>
      <trans-unit id="4">
        <source xml:lang="zh-CN">我们在<bpt id="l2">[</bpt>“动量法”<ept id="l2">]</ept><bpt id="l3">(</bpt>momentum.md<ept id="l3">)</ept>一节里介绍过指数加权移动平均。不同于Adagrad里状态变量$\boldsymbol{s}_t$是截至时间步$t$所有小批量随机梯度$\boldsymbol{g}_t$按元素平方和，RMSProp将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数$0 \leq \gamma %%%less-than%%% 1$，RMSProp在时间步$t&gt;0$计算</source>
        <target xml:lang="en-US">We introduced EWMA (exponentially weighted moving average) in the <bpt id="l2">[</bpt>"Momentum"<ept id="l2">]</ept><bpt id="l3">(</bpt>momentum.md<ept id="l3">)</ept> section. Unlike in Adagrad, the state variable $\boldsymbol{s}_t$ is the sum of the square by element all the mini-batch stochastic gradients $\boldsymbol{g}_t$ up to the time step $t$, RMSProp uses the EWMA on the square by element results of these gradients. Specifically, given the hyperparameter $0 \leq \gamma %%%less-than%%% 1$, RMSProp is computed at time step $t&gt;0$.</target>
      </trans-unit>
      <trans-unit id="5">
        <source xml:lang="zh-CN">$$\boldsymbol{s}<bpt id="3">_</bpt>t \leftarrow \gamma \boldsymbol{s}<ept id="3">_</ept>{t-1} + (1 - \gamma) \boldsymbol{g}_t \odot \boldsymbol{g}_t.</source>
        <target xml:lang="en-US">$$\boldsymbol{s}<bpt id="3">_</bpt>t \leftarrow \gamma \boldsymbol{s}<ept id="3">_</ept>{t-1} + (1 - \gamma) \boldsymbol{g}_t \odot \boldsymbol{g}_t.</target>
      </trans-unit>
      <trans-unit id="6">
        <source xml:lang="zh-CN">$$</source>
        <target xml:lang="en-US">$$</target>
      </trans-unit>
      <trans-unit id="7">
        <source xml:lang="zh-CN">和Adagrad一样，RMSProp将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量</source>
        <target xml:lang="en-US">Like Adagrad, RMSProp re-adjusts the learning rate of each element in the independent variable of the objective function with element operations and then updates the independent variable.</target>
      </trans-unit>
      <trans-unit id="8">
        <source xml:lang="zh-CN">$$\boldsymbol{x}<bpt id="3">_</bpt>t \leftarrow \boldsymbol{x}<ept id="3">_</ept>{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t, $$</source>
        <target xml:lang="en-US">$$\boldsymbol{x}<bpt id="3">_</bpt>t \leftarrow \boldsymbol{x}<ept id="3">_</ept>{t-1} - \frac{\eta}{\sqrt{\boldsymbol{s}_t + \epsilon}} \odot \boldsymbol{g}_t, $$</target>
      </trans-unit>
      <trans-unit id="9">
        <source xml:lang="zh-CN">其中$\eta$是学习率，$\epsilon$是为了维持数值稳定性而添加的常数，例如$10^{-6}$。因为RMSProp的状态变量是对平方项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$的指数加权移动平均，所以可以看作是最近$1/(1-\gamma)$个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中不再一直降低（或不变）。</source>
        <target xml:lang="en-US">Here, eta$ is the learning rate while $\epsilon$ is a constant added to maintain numerical stability, such as $10^{-6}$. Because the state variable of RMSProp is an EWMA of the squared term $\boldsymbol{g}_t \odot \boldsymbol{g}_t$, it can be seen as the weighted average of the mini-batch stochastic gradient's squared terms from the last $1/(1-\gamma)$ time steps. Therefore, the learning rate of each element in the independent variable will not always decline (or remain unchanged) during iteration.</target>
      </trans-unit>
      <trans-unit id="10">
        <source xml:lang="zh-CN">照例，让我们先观察RMSProp对目标函数$f(\boldsymbol{x})=0.1x_1^2+2x_2^2$中自变量的迭代轨迹。回忆在<bpt id="l3">[</bpt>“Adagrad”<ept id="l3">]</ept><bpt id="l4">(</bpt>adagrad.md<ept id="l4">)</ept>一节使用学习率为0.4的Adagrad，自变量在迭代后期的移动幅度较小。但在同样的学习率下，RMSProp可以较快逼近最优解。</source>
        <target xml:lang="en-US">By convention, we will use the objective function $f(\boldsymbol{x})=0.1x_1^2+2x_2^2$ to observe the iterative trajectory of the independent variable in RMSProp. Recall that in the <bpt id="l3">[</bpt>"Adagrad"<ept id="l3">]</ept><bpt id="l4">(</bpt>adagrad.md<ept id="l4">)</ept> section, when we used Adagrad with a learning rate of 0.4, the independent variable moved less in later stages of iteration. However, at the same learning rate, RMSProp can approach the optimal solution faster.</target>
      </trans-unit>
      <trans-unit id="11">
        <source xml:lang="zh-CN">从零开始实现</source>
        <target xml:lang="en-US">Implementation from Scratch</target>
      </trans-unit>
      <trans-unit id="12">
        <source xml:lang="zh-CN">接下来按照算法中的公式实现RMSProp。</source>
        <target xml:lang="en-US">Next, we implement RMSProp with the formula in the algorithm.</target>
      </trans-unit>
      <trans-unit id="13">
        <source xml:lang="zh-CN">我们将初始学习率设为0.01，并将超参数$\gamma$设为0.9。此时，变量$\boldsymbol{s}_t$可看作是最近$1/(1-0.9) = 10$个时间步的平方项$\boldsymbol{g}_t \odot \boldsymbol{g}_t$的加权平均。</source>
        <target xml:lang="en-US">We set the initial learning rate to 0.01 and the hyperparameter $\gamma$ to 0.9. Now, the variable $\boldsymbol{s}_t$ can be treated as the weighted average of the square term $\boldsymbol{g}_t \odot \boldsymbol{g}_t$ from the last $1/(1-0.9) = 10$ time steps.</target>
      </trans-unit>
      <trans-unit id="14">
        <source xml:lang="zh-CN">Gluon实现</source>
        <target xml:lang="en-US">Implementation with Gluon</target>
      </trans-unit>
      <trans-unit id="15">
        <source xml:lang="zh-CN">通过算法名称为“rmsprop”的<bpt id="2">`</bpt>Trainer<ept id="2">`</ept>实例，我们便可使用Gluon实现的RMSProp算法来训练模型。注意超参数$\gamma$通过<bpt id="5">`</bpt>gamma1<ept id="5">`</ept>指定。</source>
        <target xml:lang="en-US">From the <bpt id="2">`</bpt>Trainer<ept id="2">`</ept> instance of the algorithm named "rmsprop", we can implement the RMSProp algorithm with Gluon to train models. Note that the hyperparameter $\gamma$ is assigned by <bpt id="5">`</bpt>gamma1<ept id="5">`</ept>.</target>
      </trans-unit>
      <trans-unit id="16">
        <source xml:lang="zh-CN">小结</source>
        <target xml:lang="en-US">Summary</target>
      </trans-unit>
      <trans-unit id="17">
        <source xml:lang="zh-CN">RMSProp和Adagrad的不同在于，RMSProp使用了小批量随机梯度按元素平方的指数加权移动平均来调整学习率。</source>
        <target xml:lang="en-US">The difference between RMSProp and Adagrad is that RMSProp uses an EWMA on the squares of elements in the mini-batch stochastic gradient to adjust the learning rate.</target>
      </trans-unit>
      <trans-unit id="18">
        <source xml:lang="zh-CN">练习</source>
        <target xml:lang="en-US">exercise</target>
      </trans-unit>
      <trans-unit id="19">
        <source xml:lang="zh-CN">把$\gamma$的值设为1，实验结果有什么变化？为什么？</source>
        <target xml:lang="en-US">What happens to the experimental results if we set the value of $\gamma$ to 1? Why?</target>
      </trans-unit>
      <trans-unit id="20">
        <source xml:lang="zh-CN">试着使用其他的初始学习率和$\gamma$超参数的组合，观察并分析实验结果。</source>
        <target xml:lang="en-US">Try using other combinations of initial learning rates and $\gamma$ hyperparameters and observe and analyze the experimental results.</target>
      </trans-unit>
      <trans-unit id="21">
        <source xml:lang="zh-CN">扫码直达<bpt id="l2">[</bpt>讨论区<ept id="l2">]</ept><bpt id="l3">(</bpt>https://discuss.gluon.ai/t/topic/2275<ept id="l3">)</ept></source>
        <target xml:lang="en-US">Scan the QR Code to Access <bpt id="l2">[</bpt>Discussions<ept id="l2">]</ept><bpt id="l3">(</bpt>https://discuss.gluon.ai/t/topic/2275<ept id="l3">)</ept></target>
      </trans-unit>
      <trans-unit id="22">
        <source xml:lang="zh-CN"><bpt id="1">![</bpt>../img/qr_rmsprop.svg<ept id="1">]</ept></source>
        <target xml:lang="en-US"><bpt id="1">![</bpt>../img/qr_rmsprop.svg<ept id="1">]</ept></target>
      </trans-unit>
      <trans-unit id="23">
        <source xml:lang="zh-CN">参考文献</source>
        <target xml:lang="en-US">Reference</target>
      </trans-unit>
      <trans-unit id="24">
        <source xml:lang="zh-CN"><bpt id="1">[</bpt>1<ept id="1">]</ept> Tieleman, T., %%%ampersand%%% Hinton, G.</source>
        <target xml:lang="en-US"><bpt id="1">[</bpt>1<ept id="1">]</ept> Tieleman, T., %%%ampersand%%% Hinton, G.</target>
      </trans-unit>
      <trans-unit id="25">
        <source xml:lang="zh-CN">(2012).</source>
        <target xml:lang="en-US">(2012).</target>
      </trans-unit>
      <trans-unit id="26">
        <source xml:lang="zh-CN">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.</source>
        <target xml:lang="en-US">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.</target>
      </trans-unit>
      <trans-unit id="27">
        <source xml:lang="zh-CN">COURSERA: Neural networks for machine learning, 4(2), 26-31.</source>
        <target xml:lang="en-US">COURSERA: Neural networks for machine learning, 4(2), 26-31.</target>
      </trans-unit>
    </body>
 </file>
</xliff>