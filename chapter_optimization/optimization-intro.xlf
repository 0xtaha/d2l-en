<?xml version='1.0' encoding='UTF-8'?>
<xliff xmlns="urn:oasis:names:tc:xliff:document:1.2" version="1.2">
  <file original="chapter_optimization/optimization-intro.md" source-language="zh-CN" target-language="en-US" datatype="markdown">
    <header>
      <skl>
        <external-file href="chapter_optimization/optimization-intro.skl.md"/>
      </skl>
    </header>
    <body>
      <trans-unit id="1">
        <source xml:lang="zh-CN">优化与深度学习</source>
        <target xml:lang="en-US">Optimization and Deep Learning</target>
      </trans-unit>
      <trans-unit id="2">
        <source xml:lang="zh-CN">本节将讨论优化与深度学习的关系以及优化在深度学习中的挑战。在一个深度学习问题中，通常我们会预先定义一个损失函数。有了损失函数以后，我们就可以使用优化算法试图将其最小化。在优化中，这样的损失函数通常被称作优化问题的目标函数（objective function）。依据惯例，优化算法通常只考虑最小化目标函数。其实，任何最大化问题都可以很容易地转化为最小化问题：我们只需令目标函数的相反数为新的目标函数。</source>
        <target xml:lang="en-US">In this section, we will discuss the relationship between optimization and deep learning as well as the challenges of using optimization in deep learning. For a deep learning problem, we will usually define a loss function first. Once we have the loss function, we can use an optimization algorithm in attempt to minimize the loss. In optimization, a loss function is often referred to as the objective function of the optimization problem. Traditionally, optimization algorithms usually only consider minimizing the objective function. In fact, any maximization problem can be easily transformed into a minimization problem: we just need to use the opposite of the objective function as the new objective function.</target>
      </trans-unit>
      <trans-unit id="3">
        <source xml:lang="zh-CN">优化与深度学习的关系</source>
        <target xml:lang="en-US">The Relationship Between Optimization and Deep Learning</target>
      </trans-unit>
      <trans-unit id="4">
        <source xml:lang="zh-CN">虽然优化为深度学习提供了最小化损失函数的方法，但本质上，优化与深度学习之间的目标是有区别的。
在<bpt id="l2">[</bpt>“模型选择、欠拟合和过拟合”<ept id="l2">]</ept><bpt id="l3">(</bpt>../chapter_deep-learning-basics/underfit-overfit.md<ept id="l3">)</ept>一节中，我们区分了训练误差和泛化误差。
由于优化算法的目标函数通常是一个基于训练数据集的损失函数，优化的目标在于降低训练误差。
而深度学习的目标在于降低泛化误差。
为了降低泛化误差，除了使用优化算法降低训练误差以外，我们还需要注意应对过拟合。</source>
        <target xml:lang="en-US">Although optimization provides a way to minimize the loss function for deep learning, in essence, the goals of optimization and deep learning are different.
In the <bpt id="l2">[</bpt>"Model Selection, Underfitting and Overfitting"<ept id="l2">]</ept><bpt id="l3">(</bpt>../chapter_deep-learning-basics/underfit-overfit.md<ept id="l3">)</ept> section, we discussed the difference between the training error and generalization error.
Because the objective function of the optimization algorithm is usually a loss function based on the training data set, the goal of optimization is to reduce the training error.
However, the goal of deep learning is to reduce the generalization error.
In order to reduce the generalization error, we need to pay attention to overfitting in addition to using the optimization algorithm to reduce the training error.</target>
      </trans-unit>
      <trans-unit id="5">
        <source xml:lang="zh-CN">本章中，我们只关注优化算法在最小化目标函数上的表现，而不关注模型的泛化误差。</source>
        <target xml:lang="en-US">In this chapter, we are going to focus specifically on the performance of the optimization algorithm in minimizing the objective function, rather than the model's generalization error.</target>
      </trans-unit>
      <trans-unit id="6">
        <source xml:lang="zh-CN">优化在深度学习中的挑战</source>
        <target xml:lang="en-US">Optimization Challenges in Deep Learning</target>
      </trans-unit>
      <trans-unit id="7">
        <source xml:lang="zh-CN">我们在<bpt id="l2">[</bpt>“线性回归”<ept id="l2">]</ept><bpt id="l3">(</bpt>../chapter_deep-learning-basics/linear-regression.md<ept id="l3">)</ept>一节中对优化问题的解析解和数值解做了区分。深度学习中绝大多数的目标函数都很复杂。因此，很多优化问题并不存在解析解，而需要使用基于数值方法的优化算法找到近似解，即数值解。我们讨论的优化算法都是这类基于数值方法的算法。为了求得最小化目标函数的数值解，我们将通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。</source>
        <target xml:lang="en-US">In the <bpt id="l2">[</bpt>Linear Regression<ept id="l2">]</ept><bpt id="l3">(</bpt>../chapter_deep-learning-basics/linear-regression.md<ept id="l3">)</ept> section, we differentiated between analytical solutions and numerical solutions in optimization problems. In deep learning, most objective functions are complicated. Therefore, many optimization problems do not have analytical solutions. Instead, we must use optimization algorithms based on the numerical method to find approximate solutions, which also known as numerical solutions. The optimization algorithms discussed here are all numerical method-based algorithms. In order to minimize the numerical solution of the objective function, we will reduce the value of the loss function as much as possible by using optimization algorithms to finitely update the model parameters.</target>
      </trans-unit>
      <trans-unit id="8">
        <source xml:lang="zh-CN">优化在深度学习中有很多挑战。以下描述了其中的两个挑战：局部最小值和鞍点。为了更好地描述问题，我们先导入本节中实验需要的包或模块。</source>
        <target xml:lang="en-US">There are many challenges in deep learning optimization. Two such challenges are discussed below: local minimums and saddle points. To better describe the problem, we first import the packages or modules required for the experiments in this section.</target>
      </trans-unit>
      <trans-unit id="9">
        <source xml:lang="zh-CN">局部最小值</source>
        <target xml:lang="en-US">Local Minimums</target>
      </trans-unit>
      <trans-unit id="10">
        <source xml:lang="zh-CN">对于目标函数$f(x)$，如果$f(x)$在$x$上的值比在$x$邻近的其他点的值更小，那么$f(x)$可能是一个局部最小值（local minimum）。如果$f(x)$在$x$上的值是目标函数在整个定义域上的最小值，那么$f(x)$是全局最小值（global minimum）。</source>
        <target xml:lang="en-US">For the objective function $f(x)$, if the value of $f(x)$ at $x$ is smaller than the values of $f(x)$ at any other points in the vicinity of $x$, then $f(x)$ could be a local minimum. If the value of $f(x)$ at $x$ is the minimum of the objective function over the entire domain, then $f(x)$ is the global minimum.</target>
      </trans-unit>
      <trans-unit id="11">
        <source xml:lang="zh-CN">举个例子，给定函数</source>
        <target xml:lang="en-US">For example, given the function</target>
      </trans-unit>
      <trans-unit id="12">
        <source xml:lang="zh-CN">$$f(x) = x \cdot \text{cos}(\pi x), \qquad -1.0 \leq x \leq 2.0,$$</source>
        <target xml:lang="en-US">$$f(x) = x \cdot \text{cos}(\pi x), \qquad -1.0 \leq x \leq 2.0,$$</target>
      </trans-unit>
      <trans-unit id="13">
        <source xml:lang="zh-CN">我们可以大致找出该函数的局部最小值和全局最小值的位置。需要注意的是，图中箭头所指示的只是大致位置。</source>
        <target xml:lang="en-US">we can approximate the local minimum and global minimum of this function. Please note that the arrows in the figure only indicate the approximate positions.</target>
      </trans-unit>
      <trans-unit id="14">
        <source xml:lang="zh-CN">深度学习模型的目标函数可能有若干局部最优值。当一个优化问题的数值解在局部最优解附近时，由于目标函数有关解的梯度接近或变成零，最终迭代求得的数值解可能只令目标函数局部最小化而非全局最小化。</source>
        <target xml:lang="en-US">The objective function of the deep learning model may have several local optimums. When the numerical solution of an optimization problem is near the local optimum, the numerical solution obtained by the final iteration may only minimize the objective function locally, rather than globally, as the gradient of the objective function's solutions approaches or becomes zero.</target>
      </trans-unit>
      <trans-unit id="15">
        <source xml:lang="zh-CN">鞍点</source>
        <target xml:lang="en-US">Saddle Points</target>
      </trans-unit>
      <trans-unit id="16">
        <source xml:lang="zh-CN">刚刚我们提到，梯度接近或变成零可能是由于当前解在局部最优解附近所造成的。事实上，另一种可能性是当前解在鞍点（saddle point）附近。举个例子，给定函数</source>
        <target xml:lang="en-US">As we just mentioned, one possible explanation for a gradient that approaches or becomes zero is that the current solution is close to a local optimum. In fact, there is another possibility. The current solution could be near a saddle point. For example, given the function</target>
      </trans-unit>
      <trans-unit id="17">
        <source xml:lang="zh-CN">$$f(x) = x^3,$$</source>
        <target xml:lang="en-US">$$f(x) = x^3,$$</target>
      </trans-unit>
      <trans-unit id="18">
        <source xml:lang="zh-CN">我们可以找出该函数的鞍点位置。</source>
        <target xml:lang="en-US">we can find the position of the saddle point of this function.</target>
      </trans-unit>
      <trans-unit id="19">
        <source xml:lang="zh-CN">再举个定义在二维空间的函数的例子，例如</source>
        <target xml:lang="en-US">Now, we will use another example of a two-dimensional function, defined as follows:</target>
      </trans-unit>
      <trans-unit id="20">
        <source xml:lang="zh-CN">$$f(x, y) = x^2 - y^2.$$</source>
        <target xml:lang="en-US">$$f(x, y) = x^2 - y^2.$$</target>
      </trans-unit>
      <trans-unit id="21">
        <source xml:lang="zh-CN">我们可以找出该函数的鞍点位置。也许你已经发现了，该函数看起来像一个马鞍，而鞍点恰好是马鞍上可坐区域的中心。</source>
        <target xml:lang="en-US">We can find the position of the saddle point of this function. Perhaps you have noticed that the function looks just like a saddle, and the saddle point happens to be the center point of the seat.</target>
      </trans-unit>
      <trans-unit id="22">
        <source xml:lang="zh-CN">在上图的鞍点位置，目标函数在$x$轴方向上是局部最小值，而在$y$轴方向上是局部最大值。</source>
        <target xml:lang="en-US">In the figure above, the objective function's local minimum and local maximum can be found on the $x$ axis and $y$ axis respectively at the position of the saddle point.</target>
      </trans-unit>
      <trans-unit id="23">
        <source xml:lang="zh-CN">假设一个函数的输入为$k$维向量，输出为标量，那么它的黑塞矩阵（Hessian matrix）有$k$个特征值（参见<bpt id="l2">[</bpt>“数学基础”<ept id="l2">]</ept><bpt id="l3">(</bpt>../chapter_appendix/math.md<ept id="l3">)</ept>一节）。该函数在梯度为零的位置上可能是局部最小值、局部最大值或者鞍点：</source>
        <target xml:lang="en-US">We assume that the input of a function is a $k$-dimensional vector and its output is a scalar, so its Hessian matrix will have $k$ eigenvalues (see the <bpt id="l2">[</bpt>"Mathematical Foundation"<ept id="l2">]</ept><bpt id="l3">(</bpt>../chapter_appendix/math.md<ept id="l3">)</ept> section). The solution of the function could be a local minimum, a local maximum, or a saddle point at a position where the function gradient is zero:</target>
      </trans-unit>
      <trans-unit id="24">
        <source xml:lang="zh-CN">当函数的黑塞矩阵在梯度为零的位置上的特征值全为正时，该函数得到局部最小值。</source>
        <target xml:lang="en-US">When the eigenvalues of the function's Hessian matrix at the zero-gradient position are all positive, we have a local minimum for the function.</target>
      </trans-unit>
      <trans-unit id="25">
        <source xml:lang="zh-CN">当函数的黑塞矩阵在梯度为零的位置上的特征值全为负时，该函数得到局部最大值。</source>
        <target xml:lang="en-US">When the eigenvalues of the function's Hessian matrix at the zero-gradient position are all negative, we have a local maximum for the function.</target>
      </trans-unit>
      <trans-unit id="26">
        <source xml:lang="zh-CN">当函数的黑塞矩阵在梯度为零的位置上的特征值有正有负时，该函数得到鞍点。</source>
        <target xml:lang="en-US">When the eigenvalues of the function's Hessian matrix at the zero-gradient position are negative and positive, we have a saddle point for the function.</target>
      </trans-unit>
      <trans-unit id="27">
        <source xml:lang="zh-CN">随机矩阵理论告诉我们，对于一个大的高斯随机矩阵来说，任一特征值是正或者是负的概率都是0.5 <bpt id="2">[</bpt>1<ept id="2">]</ept>。那么，以上第一种情况的概率为 $0.5^k$。由于深度学习模型参数通常都是高维的（$k$很大），目标函数的鞍点通常比局部最小值更常见。</source>
        <target xml:lang="en-US">The random matrix theory tells us that, for a large Gaussian random matrix, the probability for any eigenvalue to be positive or negative is 0.5<bpt id="2">[</bpt>1<ept id="2">]</ept>. Thus, the probability of the first case above is $0.5^k$. Since, generally, the parameters of deep learning models are high-dimensional ($k$ is large), saddle points in the objective function are more commonly seen than local minimums.</target>
      </trans-unit>
      <trans-unit id="28">
        <source xml:lang="zh-CN">深度学习中，虽然找到目标函数的全局最优解很难，但这并非必要。我们将在本章接下来的小节中逐一介绍深度学习中常用的优化算法，它们在很多实际问题中都训练出了十分有效的深度学习模型。</source>
        <target xml:lang="en-US">In deep learning, it is difficult, but also not necessary, to find the global optimal solution of the objective function. In the subsequent sections of this chapter, we will introduce the optimization algorithms commonly used in deep learning one by one. These algorithms have trained some very effective deep learning models that have tackled practical problems.</target>
      </trans-unit>
      <trans-unit id="29">
        <source xml:lang="zh-CN">小结</source>
        <target xml:lang="en-US">Summary</target>
      </trans-unit>
      <trans-unit id="30">
        <source xml:lang="zh-CN">由于优化算法的目标函数通常是一个基于训练数据集的损失函数，优化的目标在于降低训练误差。</source>
        <target xml:lang="en-US">Because the objective function of the optimization algorithm is usually a loss function based on the training data set, the goal of optimization is to reduce the training error.</target>
      </trans-unit>
      <trans-unit id="31">
        <source xml:lang="zh-CN">由于深度学习模型参数通常都是高维的，目标函数的鞍点通常比局部最小值更常见。</source>
        <target xml:lang="en-US">Since, generally, the parameters of deep learning models are high-dimensional, saddle points in the objective function are more commonly seen than local minimums.</target>
      </trans-unit>
      <trans-unit id="32">
        <source xml:lang="zh-CN">练习</source>
        <target xml:lang="en-US">exercise</target>
      </trans-unit>
      <trans-unit id="33">
        <source xml:lang="zh-CN">对于深度学习中的优化问题，你还能想到哪些其他的挑战？</source>
        <target xml:lang="en-US">What other challenges involved in deep learning optimization can you think of?</target>
      </trans-unit>
      <trans-unit id="34">
        <source xml:lang="zh-CN">扫码直达<bpt id="l2">[</bpt>讨论区<ept id="l2">]</ept><bpt id="l3">(</bpt>https://discuss.gluon.ai/t/topic/1876<ept id="l3">)</ept></source>
        <target xml:lang="en-US">Scan the QR Code to Access <bpt id="l2">[</bpt>Discussions<ept id="l2">]</ept><bpt id="l3">(</bpt>https://discuss.gluon.ai/t/topic/1876<ept id="l3">)</ept></target>
      </trans-unit>
      <trans-unit id="35">
        <source xml:lang="zh-CN"><bpt id="1">![</bpt>../img/qr_optimization-intro.svg<ept id="1">]</ept></source>
        <target xml:lang="en-US"><bpt id="1">![</bpt>../img/qr_optimization-intro.svg<ept id="1">]</ept></target>
      </trans-unit>
      <trans-unit id="36">
        <source xml:lang="zh-CN">参考文献</source>
        <target xml:lang="en-US">Reference</target>
      </trans-unit>
      <trans-unit id="37">
        <source xml:lang="zh-CN"><bpt id="1">[</bpt>1<ept id="1">]</ept> Wigner, E.</source>
        <target xml:lang="en-US"><bpt id="1">[</bpt>1<ept id="1">]</ept> Wigner, E.</target>
      </trans-unit>
      <trans-unit id="38">
        <source xml:lang="zh-CN">P.</source>
        <target xml:lang="en-US">P.</target>
      </trans-unit>
      <trans-unit id="39">
        <source xml:lang="zh-CN">(1958).</source>
        <target xml:lang="en-US">(1958).</target>
      </trans-unit>
      <trans-unit id="40">
        <source xml:lang="zh-CN">On the distribution of the roots of certain symmetric matrices.</source>
        <target xml:lang="en-US">On the distribution of the roots of certain symmetric matrices.</target>
      </trans-unit>
      <trans-unit id="41">
        <source xml:lang="zh-CN">Annals of Mathematics, 325-327.</source>
        <target xml:lang="en-US">Annals of Mathematics, 325-327.</target>
      </trans-unit>
    </body>
 </file>
</xliff>