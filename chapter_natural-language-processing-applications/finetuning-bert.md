# Fine-Tuning BERT for Sequence-Level and Token-Level Applications
:label:`sec_finetuning-bert`

*This section is under construction.*



![Fine-tuning BERT for single text classification tasks, such as sentiment analysis.](../img/bert-one-seq.svg)
:label:`fig_bert-one-seq`

...
![Fine-tuning BERT for text pair classification tasks, such as natural language inference.](../img/bert-two-seqs.svg)
:label:`fig_bert-two-seqs`

...
![Fine-tuning BERT for text tagging tasks, such as part-of-speech tagging](../img/bert-tagging.svg)
:label:`fig_bert-tagging`

...
![Fine-tuning BERT for question answering](../img/bert-qa.svg)
:label:`fig_bert-qa`
...

## Summary

* Fine-tune BERT.

## Exercises

1. Suppose that we want to design a search engine algorithm for news articles. When the system receives an query (e.g., "oil industry in coronavirus crisis"), it should return a ranked list of news articles that are most relevant to the query. What data do you need to collect? How can we use BERT in the algorithm design?
1. How can we leverage BERT in text generation tasks such as machine translation?



## [Discussions](https://discuss.mxnet.io/t/5882)

![](../img/qr_finetuning-bert.svg)
